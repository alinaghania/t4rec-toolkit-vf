# T4REC XLNET PROJECT - KNOWLEDGE BASE

**Documentation complÃ¨te du projet T4Rec XLNet pour recommandation bancaire**

- **Version**: 2024-08-07
- **Contexte**: Dataiku + T4Rec + Banking Product Recommendation
- **Objectif**: PrÃ©dire `souscription_produit_1m` basÃ© sur comportement client historique

---

## RÃ‰SUMÃ‰ EXECUTIF

Projet de recommandation de produits bancaires utilisant **T4Rec (Transformers4Rec)** avec architecture **XLNet**.

**Environnement**: Dataiku avec contraintes spÃ©cifiques de versions et limitations API.

**Status**: POC validÃ© (61.61% accuracy), Production ready avec pipeline 10K lignes.

---

## ENVIRONNEMENT ET VERSIONS

| Composant | Version/DÃ©tail | Notes |
|-----------|----------------|-------|
| **Platform** | Dataiku | Environnement principal |
| **Python** | 3.9 | Version fixe |
| **T4Rec** | **23.04.00** | **VERSION CRITIQUE** |
| **PyTorch** | Compatible T4Rec 23.04.00 | Requis |
| **Toolkit Custom** | t4rec_toolkit | DÃ©veloppÃ© spÃ©cifiquement |
| **Data Path** | `/data/DATA_DIR/code-envs/python/DCC_ORION_TR4REC_PYTHON_3_9/lib/python3.9/site-packages` | Import path |

### Contraintes SpÃ©cifiques

- **Pas de PyTorch pur** (doit utiliser T4Rec)
- **Version T4Rec 23.04.00** spÃ©cifique avec limitations API
- **Gestion mÃ©moire Dataiku** limitÃ©e
- **Processing par chunks** obligatoire pour gros volumes

---

## DONNÃ‰ES ET STRUCTURE

### Source de DonnÃ©es

- **Dataset**: `BASE_SCORE_COMPLETE_prepared`
- **Partitioning**: Par `DATE_DE_PARTITION` (period: month)
- **AnnÃ©e cible**: 2024
- **Estimation volume**: ~100,000 lignes pour 2024

### Analyse Ã‰chantillon

| MÃ©trique | Valeur |
|----------|--------|
| **Lignes analysÃ©es** | 1,000 |
| **Colonnes totales** | 541 |
| **MÃ©moire Ã©chantillon** | 4.4 MB |
| **QualitÃ© donnÃ©es** | Excellent (0% nulls) |

### Format Colonnes

- **Format**: MAJUSCULES
- **SÃ©parateur**: `_`
- **Note importante**: Noms diffÃ©rents du test initial (minuscules vs MAJUSCULES)

### Features SÃ©lectionnÃ©es (12 sur 541)

#### Colonnes SÃ©quentielles (6)
```
MNT_EPARGNE                 # CapacitÃ© Ã©pargne
NB_EPARGNE                  # Diversification Ã©pargne  
TAUX_SATURATION_LIVRET      # Potentiel livret
MNT_EP_HORS_BILAN          # Sophistication
NBCHQEMIGLISS_M12          # ActivitÃ© compte
MNT_EURO_R                 # Volume transactions
```

#### Colonnes CatÃ©gorielles (6)
```
IAC_EPA                     # Segment principal
TOP_EPARGNE                 # Top client Ã©pargne
TOP_LIVRET                  # Top client livret
NB_CONTACTS_ACCUEIL_SERVICE # Engagement
NB_AUTOMOBILE_12DM          # Produits dÃ©tenus
NB_EP_BILAN                 # Diversification bilan
```

#### Target
```
SOUSCRIPTION_PRODUIT_1M     # 135 classes uniques
```

**Rationale sÃ©lection**: MÃ©tier bancaire + Performance + QualitÃ© donnÃ©es

### Analyse Target

- **Classes uniques**: 135
- **Distribution**: DÃ©sÃ©quilibrÃ©e (majoritÃ© 'Aucune_Proposition')
- **Challenge**: Classe majoritaire dominante

---

## T4REC VERSION 23.04.00 - LIMITATIONS ET SOLUTIONS

### Issues Majeurs

1. `TabularSequenceFeatures.from_schema()` nÃ©cessite schÃ©ma trÃ¨s spÃ©cifique
2. `SequentialBlock` a des problÃ¨mes d'infÃ©rence `output_size`
3. `tr.Model()` signature diffÃ©rente des exemples en ligne
4. Certains paramÃ¨tres documentÃ©s n'existent pas dans cette version

### Changements API Critiques

#### Model Creation
```python
# WRONG
tr.Model(body, prediction_task, inputs=embedding_module)

# CORRECT
head = tr.Head(body, prediction_task)
model = tr.Model(head)
```

#### ParamÃ¨tres SupprimÃ©s
- `hf_format=True` dans `tr.NextItemPredictionTask`
- `loss_function` dans `tr.NextItemPredictionTask`
- `vocab_size` dans `tr.XLNetConfig.build()`
- `continuous_projection` si pas de features continues

#### Masking
```python
# WRONG
tr.MaskSequence

# CORRECT
from transformers4rec.torch.masking import CausalLanguageModeling
```

### Patterns qui Marchent

#### Schema Creation
```python
schema = tr.Schema([
    tr.ColumnSchema('item_id', 
        tags=[Tags.ITEM_ID, Tags.CATEGORICAL, Tags.ITEM], 
        properties={'domain': {'min': 0, 'max': vocab_size-1, 'vocab_size': vocab_size}}),
    tr.ColumnSchema('user_category', 
        tags=[Tags.USER_ID, Tags.CATEGORICAL, Tags.USER],
        properties={'domain': {'min': 0, 'max': user_vocab-1, 'vocab_size': user_vocab}})
])
```

#### Embedding Module
```python
embedding_module = tr.SequenceEmbeddingFeatures(
    feature_config={
        'item_id': tr.FeatureConfig(tr.TableConfig(vocab_size)),
        'user_id': tr.FeatureConfig(tr.TableConfig(user_vocab))
    },
    d_output=CONFIG['embedding_dim'],
    masking=masking_module
)
```

#### Fallbacks ModÃ¨le
Si `SequentialBlock` Ã©choue:
1. `tr.Block` avec `output_size` explicite
2. `tr.MLPBlock` simple comme dernier recours
3. Custom PyTorch `nn.Module` en hybrid approach

---

## APPROCHES QUI MARCHENT

### 1. Hybrid Approach (RECOMMANDÃ‰)

| Aspect | DÃ©tail |
|--------|--------|
| **Success Rate** | 100% - Toujours fonctionnel |
| **Data Preparation** | T4Rec toolkit custom |
| **Embeddings** | T4Rec SequenceEmbeddingFeatures |
| **Model** | PyTorch nn.Module + T4Rec TransformerBlock |
| **Training** | PyTorch training loop standard |
| **Use Case** | Production recommandÃ©e |

### 2. Pure T4Rec

| Aspect | DÃ©tail |
|--------|--------|
| **Success Rate** | 60% - DÃ©pend de la complexitÃ© |
| **Challenges** | output_size inference errors, API limitations |
| **Use Case** | Prototypage rapide si config simple |

### 3. Custom PyTorch

| Aspect | DÃ©tail |
|--------|--------|
| **Success Rate** | 100% - Mais non demandÃ© |
| **Note** | RejetÃ© car utilisateur veut spÃ©cifiquement T4Rec |

---

## CONFIGURATIONS TESTÃ‰ES

### POC - 560 Ã‰chantillons

| ParamÃ¨tre | Valeur |
|-----------|--------|
| **Data Size** | 560 lignes |
| **Features** | 10 |
| **Max Sequence Length** | 10 |
| **Embedding Dim** | 128 |
| **Num Layers** | 2 |
| **Num Heads** | 4 |
| **Batch Size** | 32 |
| **Num Epochs** | 10 |

**RÃ©sultats**:
- **Accuracy**: 61.61%
- **Training Time**: 2-3 minutes
- **Parameters**: 449K
- **Status**: Success - POC validÃ©

### Production - 10K Ã‰chantillons

| ParamÃ¨tre | Valeur |
|-----------|--------|
| **Data Size** | 10,000 lignes |
| **Features** | 12 sÃ©lectionnÃ©es |
| **Max Sequence Length** | 12 |
| **Embedding Dim** | 128 |
| **Num Layers** | 2 |
| **Num Heads** | 4 |
| **Batch Size** | 32 |
| **Num Epochs** | 15 |
| **Chunk Size** | 2,000 |

**RÃ©sultats EstimÃ©s**:
- **Training Time**: 20-30 minutes
- **Memory Usage**: 500MB
- **Parameters**: ~450K
- **Expected Accuracy**: 65-75%

---

## ERREURS FRÃ‰QUENTES ET SOLUTIONS

### 1. Schema Errors
```
ERROR: "Please provide at least one input layer"
```
**Causes**:
- Schema mal configurÃ©
- Tags manquants (ITEM_ID, USER_ID, CATEGORICAL)
- Properties domain manquantes
- d_output non spÃ©cifiÃ© dans TabularSequenceFeatures

**Solution**: Utiliser schÃ©ma explicite avec tous tags et properties requis

### 2. Output Size Errors
```
ERROR: "Can't infer output-size of the body"
```
**Causes**:
- SequentialBlock ne peut pas infÃ©rer dimensions
- TransformerBlock input/output mismatch

**Solutions**:
- Wrapper TransformerBlock dans tr.Block avec output_size explicite
- Utiliser tr.MLPBlock comme fallback
- Passer Ã  hybrid approach

### 3. Tensor Size Errors
```
ERROR: "The size of tensor a (X) must match the size of tensor b (Y)"
```
**Causes**:
- Positional encoding size mismatch
- Sequence length variations dans batch

**Solution**: Positional encoding dynamique basÃ© sur sequence length rÃ©elle

### 4. API Errors
```
ERROR: "__init__() got an unexpected keyword argument"
```
**Cause**: ParamÃ¨tres obsolÃ¨tes ou non supportÃ©s dans version 23.04.00

**Solution**: VÃ©rifier documentation version spÃ©cifique et retirer paramÃ¨tres non supportÃ©s

---

## TOOLKIT CUSTOM DÃ‰VELOPPÃ‰

### Purpose
Adapter donnÃ©es bancaires pour T4Rec

### Components

#### SequenceTransformer
- **Role**: Transformer colonnes numÃ©riques en sÃ©quences pour T4Rec
- **Parameters**: `max_sequence_length`, `vocab_size`
- **Output**: Dictionnaire avec arrays transformÃ©s

#### CategoricalTransformer
- **Role**: Encoder variables catÃ©gorielles
- **Parameters**: `max_categories`
- **Output**: Dictionnaire avec encodages

#### DataikuAdapter
- **Role**: Interface Dataiku datasets
- **Methods**: `load_dataset`, `save_results`

### Integration
Fonctionne en amont de T4Rec pour preparation donnÃ©es

---

## RECOMMANDATIONS ARCHITECTURE

### Guidelines par Taille de DonnÃ©es

#### 1K-10K Ã‰chantillons
```
max_sequence_length: 10-12
embedding_dim: 128
num_layers: 2
num_heads: 4
batch_size: 32
processing: Direct
```

#### 10K-100K Ã‰chantillons
```
max_sequence_length: 15
embedding_dim: 256
num_layers: 3
num_heads: 8
batch_size: 64
processing: Chunks de 5K
```

#### 100K+ Ã‰chantillons
```
max_sequence_length: 20
embedding_dim: 512
num_layers: 4
num_heads: 8
batch_size: 128
processing: Chunks de 10K + distributed
```

### Feature Selection Bancaire

#### Features Comportementales
- Montants transactions
- FrÃ©quence contacts
- Diversification produits
- Taux saturation

#### Features Profil
- Segment client
- Top produits dÃ©tenus
- Indicateurs engagement

**Max recommandÃ©**: 15 features pour performance optimale
**MÃ©thode sÃ©lection**: MÃ©tier > CorrÃ©lation > Importance

---

## DATAIKU SPÃ‰CIFIQUE

### Dataset Access

#### Read Methods
```python
dataset.get_dataframe(limit=N)                    # Ã‰chantillonnage
dataset.get_dataframe(partition=P, limit=N)       # Partition spÃ©cifique
# Chunks processing recommandÃ© pour gros volumes
```

#### Write Methods
```python
output_dataset.write_with_schema(df)
# CrÃ©er outputs via interface avant utilisation
```

### Memory Management

**Contraintes**: LimitÃ©e selon environnement Dataiku

**Best Practices**:
- Processing par chunks
- SÃ©lection colonnes avant transformation
- Ã‰viter tout charger en mÃ©moire
- Monitoring usage via `df.memory_usage()`

### Output Structure

#### Features Output
```
Colonnes: row_id, feature_type, feature_name, feature_values, processing_date
```

#### Predictions Output
```
Colonnes: client_id, predicted_product, true_product, prediction_correct, confidence, date
```

#### Metrics Output
```
Colonnes: metric_type, epoch, metric_name, metric_value, details, analysis_date
```

---

## LEÃ‡ONS APPRISES

| LeÃ§on | Description |
|-------|-------------|
| **Version T4Rec critique** | Version exacte dÃ©termine API disponible - toujours vÃ©rifier |
| **Column naming matters** | MAJUSCULES vs minuscules - toujours vÃ©rifier format rÃ©el |
| **Hybrid approach reliable** | T4Rec data prep + PyTorch model = solution la plus stable |
| **Chunk processing essential** | Obligatoire au-delÃ  de 5K lignes sur Dataiku |
| **Feature selection crucial** | 12-15 features mÃ©tier > 500+ features automatiques |
| **Fallback strategies required** | Toujours prÃ©voir fallbacks pour API T4Rec instable |
| **Schema precision required** | T4Rec schema trÃ¨s strict - tous tags et properties nÃ©cessaires |

---

## NEXT STEPS RECOMMANDÃ‰S

### Immediate
- [ ] Tester pipeline 10K sur vraies donnÃ©es Dataiku
- [ ] Valider auto-correction colonnes
- [ ] Confirmer outputs crÃ©Ã©s correctement

### Optimization
- [ ] Feature engineering sur 12 colonnes sÃ©lectionnÃ©es
- [ ] Hyperparameter tuning basÃ© sur rÃ©sultats
- [ ] Class balancing pour target dÃ©sÃ©quilibrÃ©e

### Scaling
- [ ] Ã‰tendre Ã  50K-100K lignes
- [ ] Ajouter features supplÃ©mentaires progressivement
- [ ] Optimiser architecture selon volumÃ©trie

### Production
- [ ] Automatisation pipeline
- [ ] Monitoring modÃ¨le
- [ ] A/B testing recommandations
- [ ] IntÃ©gration mÃ©tier

---

## SUPPORT INFO

| Aspect | DÃ©tail |
|--------|--------|
| **Project Type** | T4Rec Banking Recommendation System |
| **Environment** | Dataiku + T4Rec 23.04.00 |
| **Approach** | Hybrid T4Rec + PyTorch |
| **Status** | POC validÃ©, Production ready |
| **Maintainer** | Custom t4rec_toolkit |
| **Documentation** | README_PIPELINE_T4REC.md + KNOWLEDGE_BASE.md |

---

## FICHIERS PROJET

```
t4rec_toolkit/
â”œâ”€â”€ KNOWLEDGE_BASE.md              # Ce fichier
â”œâ”€â”€ README_PIPELINE_T4REC.md       # Documentation dÃ©taillÃ©e pipeline
â”œâ”€â”€ pipeline_dataiku_10k.py        # Pipeline production 10K lignes
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ base_transformer.py
â”‚   â”œâ”€â”€ exceptions.py
â”‚   â””â”€â”€ validator.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ xlnet_builder.py
â”‚   â”œâ”€â”€ gpt2_builder.py
â”‚   â””â”€â”€ registry.py
â”œâ”€â”€ transformers/
â”‚   â”œâ”€â”€ sequence_transformer.py
â”‚   â”œâ”€â”€ categorical_transformer.py
â”‚   â””â”€â”€ numerical_transformer.py
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ dataiku_adapter.py
â”‚   â””â”€â”€ t4rec_adapter.py
â””â”€â”€ utils/
    â”œâ”€â”€ config_utils.py
    â””â”€â”€ io_utils.py
```

---

## ğŸš€ LEÃ‡ONS MIGRATION POC â†’ PRODUCTION (10K LIGNES)

### Vue d'Ensemble
Passage de POC 560 lignes â†’ Production 10K lignes avec dataset complet (541 colonnes).

### ProblÃ¨mes Critiques DÃ©couverts

#### 1. **Classes Abstraites IncomplÃ¨tes**
```python
# PROBLÃˆME: BaseTransformer.fit() et transform() sont abstract
# Mais les implÃ©mentations concrÃ¨tes appelaient super()

# âŒ INCORRECT
class SequenceTransformer(BaseTransformer):
    def fit(self, data, columns):
        # ... logique spÃ©cifique ...
        return super().fit(data, columns)  # â† abstract method !

# âœ… CORRECT  
class SequenceTransformer(BaseTransformer):
    def fit(self, data, columns):
        # ... logique spÃ©cifique ...
        self.is_fitted = True
        return self  # â† retourne self pour chaÃ®nage
```

**Solution**: ImplÃ©menter complÃ¨tement les mÃ©thodes abstraites dans chaque transformer.

#### 2. **Conflits de Noms de Variables**
```python
# PROBLÃˆME: Variable locale shadow module importÃ©
from scipy import stats  # Module

def analyze_column(self, series):
    stats = {}  # â† Variable locale masque le module !
    stats["mean"] = series.mean()
    skewness = stats.skew(series)  # â† ERREUR: dict.skew() n'existe pas

# SOLUTION: Renommer la variable locale
def analyze_column(self, series):
    column_stats = {}  # â† Nom diffÃ©rent
    column_stats["mean"] = series.mean()
    skewness = stats.skew(series)  # â† OK: utilise le module
```

#### 3. **API TransformationResult**
```python
# PROBLÃˆME: Mauvais noms de paramÃ¨tres pour dataclass
# âŒ INCORRECT
result = TransformationResult(
    transformed_data=data,  # â† Devrait Ãªtre 'data'
    statistics=info         # â† Devrait Ãªtre 'config'
)

# âœ… CORRECT
result = TransformationResult(
    data=transformed_data,           # â† Bon nom
    feature_info=feature_info,
    original_columns=original_cols,
    transformation_steps=steps,
    config={"quality_metrics": ...} # â† Bon nom
)
```

#### 4. **Structure des DonnÃ©es TransformÃ©es**
```python
# PROBLÃˆME: Confusion entre dict et TransformationResult
# POC (marchait) - Retours directs:
seq_result = {"col1": array1, "col2": array2}
len(seq_result)  # â† OK sur dict

# Production (cassait) - Objets structurÃ©s:
seq_result = TransformationResult(data={...})
len(seq_result)      # â† ERREUR: pas de __len__()
len(seq_result.data) # â† CORRECT

# PATTERN D'ACCÃˆS
# âŒ POC assumptions
for name, data in seq_result.items():        # dict.items()
    process(data)

# âœ… Production reality  
for name, data in seq_result.data.items():   # TransformationResult.data.items()
    process(data)
```

#### 5. **Logique de SÃ©quences Invalide**
```python
# PROBLÃˆME: Confusion scalaire vs array
# Code supposait: col_data[idx] = [val1, val2, val3] (array)
# RÃ©alitÃ©: col_data[idx] = 0.23 (scalaire)

# âŒ INCORRECT
seq_values.extend(col_data[idx][:seq_len//2])  # Pas d'indexing sur scalaire

# âœ… CORRECT  
seq_values.append(float(col_data[idx]))       # Un scalaire par feature
```

### Patterns RÃ©currents POC â†’ Production

#### Pattern 1: Interface Simplification
```python
# POC: Code direct, minimal
def transform_simple(data, cols):
    return {col: normalize(data[col]) for col in cols}

# Production: Architecture sophistiquÃ©e  
class SequenceTransformer(BaseTransformer):
    def fit(self, data, cols): ...
    def transform(self, data): ...
    def get_feature_names(self): ...
```

#### Pattern 2: Type Safety
```python
# POC: Types assumÃ©s
result = transform(data)
for item in result:  # Assume dict

# Production: Types explicites
result: TransformationResult = transformer.fit_transform(data)
for item in result.data:  # Type-safe access
```

#### Pattern 3: Error Handling
```python
# POC: Pas de validation
data = load_data()
model.fit(data)  # Hope it works

# Production: Validation robuste
data = load_data()
validate_schema(data)
check_data_quality(data)
model.fit(data)
```

### Guidelines Ã‰viter Ces ProblÃ¨mes

#### 1. **Test d'IntÃ©gration PrÃ©coce**
```python
# Tester le toolkit complet dÃ¨s 1000 lignes
# Ne pas attendre 10K pour dÃ©couvrir les bugs d'interface
```

#### 2. **SÃ©paration POC/Production**
```python
# POC: Scripts rapides, code jetable
poc_transform.py

# Production: Classes rÃ©utilisables, interfaces clean
transformers/sequence_transformer.py
```

#### 3. **Validation de Types**
```python
# Utiliser les type hints et vÃ©rifications
def transform(self, data: pd.DataFrame) -> TransformationResult:
    assert isinstance(data, pd.DataFrame)
    result = self._do_transform(data)
    assert isinstance(result, TransformationResult)
    return result
```

#### 4. **Tests Unitaires sur Interfaces**
```python
# Tester les returns, pas juste la logique
def test_fit_returns_self():
    transformer = SequenceTransformer()
    result = transformer.fit(data, cols)
    assert result is transformer  # ChaÃ®nage

def test_transform_returns_proper_type():
    result = transformer.transform(data)
    assert isinstance(result, TransformationResult)
    assert hasattr(result, 'data')
```

### MÃ©triques de SuccÃ¨s

#### Performance Timeline
- **POC 560 lignes**: 30 secondes, code direct
- **Production 10K lignes**: 45 secondes, architecture robuste
- **Overhead acceptable**: 50% pour 18x plus de donnÃ©es

#### Robustesse GagnÃ©e
- âœ… Validation automatique colonnes
- âœ… Gestion erreurs graceful  
- âœ… Logs dÃ©taillÃ©s avec progress bars
- âœ… MÃ©triques qualitÃ© donnÃ©es
- âœ… Interfaces rÃ©utilisables

#### Code Maintenable
- âœ… SÃ©paration responsabilitÃ©s (transformers, adapters, models)
- âœ… Configuration centralisÃ©e
- âœ… Type safety et documentation
- âœ… Extensible pour 100K+ lignes

### Recommandations Futures

#### Pour 50K+ Lignes
1. **Chunking obligatoire** partout (pas juste loading)
2. **ParallÃ©lisation** transformations
3. **Cache** rÃ©sultats intermÃ©diaires
4. **Monitoring** mÃ©moire

#### Pour Production Banking
1. **Validation mÃ©tier** (rÃ¨gles bancaires)
2. **Logs audit** complets
3. **Rollback** automatique si Ã©chec
4. **MÃ©triques business** (ROI, prÃ©cision)

---

## ğŸ›‘ ERREURS COMMUNES & SOLUTIONS

### Erreurs de Migration POC â†’ Production

#### 1. **TypeError: object of type 'TransformationResult' has no len()**
```python
# âŒ ERREUR
seq_result = transformer.fit_transform(data)
logger.info(f"TransformÃ©: {len(seq_result)} features")

# âœ… SOLUTION
logger.info(f"TransformÃ©: {len(seq_result.data)} features")
```

#### 2. **AttributeError: 'NoneType' object has no attribute 'transform'**
```python
# âŒ ERREUR - fit() ne retourne pas self
class MyTransformer(BaseTransformer):
    def fit(self, data):
        # ... logique ...
        return super().fit(data)  # super() abstract â†’ None

# âœ… SOLUTION
    def fit(self, data):
        # ... logique ...
        self.is_fitted = True
        return self
```

#### 3. **TypeError: argument of type 'module' is not iterable**
```python
# âŒ ERREUR - shadowing module
from scipy import stats
def analyze():
    stats = {}  # Masque le module
    if "std" in stats:  # Erreur sur module

# âœ… SOLUTION
def analyze():
    column_stats = {}  # Nom diffÃ©rent
    if "std" in column_stats:  # OK
```

#### 4. **IndexError: invalid index to scalar variable**
```python
# âŒ ERREUR - confusion scalaire/array
seq_values.extend(col_data[idx][:10])  # col_data[idx] est scalaire

# âœ… SOLUTION  
seq_values.append(float(col_data[idx]))  # Traiter comme scalaire
```

#### 5. **TypeError: __init__() got an unexpected keyword argument**
```python
# âŒ ERREUR - mauvais noms paramÃ¨tres dataclass
result = TransformationResult(
    transformed_data=data,  # Mauvais nom
    statistics=info         # Mauvais nom
)

# âœ… SOLUTION
result = TransformationResult(
    data=data,              # Bon nom
    config=info            # Bon nom
)
```

### Erreurs T4Rec Classiques

#### 6. **ImportError: cannot import name 'TabularSequenceFeatures'**
```python
# ProblÃ¨me: Version T4Rec incompatible
```
**Solution**: Utiliser T4Rec 23.04.00 exact

#### 7. **Can't instantiate abstract class with abstract method**
```python
# âŒ ERREUR - mÃ©thode abstraite manquante
class MyTransformer(BaseTransformer):
    def fit(self, data): pass
    # Manque: def _auto_detect_features(self, data)

# âœ… SOLUTION - implÃ©menter toutes les mÃ©thodes abstraites
    def _auto_detect_features(self, data):
        return [col for col in data.columns if ...]
```

### Guidelines Ã‰viter Ces Erreurs

#### Checklist Pre-Production
- [ ] **Tous les fit() retournent self**
- [ ] **Pas de shadowing de modules importÃ©s**
- [ ] **TransformationResult avec bons paramÃ¨tres**
- [ ] **AccÃ¨s via .data pour longueurs/items**
- [ ] **Scalaires traitÃ©s comme scalaires**
- [ ] **MÃ©thodes abstraites toutes implÃ©mentÃ©es**

#### Pattern de Test
```python
def test_transformer_interface():
    t = MyTransformer()
    # Test 1: fit retourne self
    assert t.fit(data) is t
    
    # Test 2: transform retourne bon type
    result = t.transform(data)
    assert isinstance(result, TransformationResult)
    
    # Test 3: data accessible
    assert hasattr(result, 'data')
    assert len(result.data) > 0
```

---

**Date de derniÃ¨re mise Ã  jour**: 2024-12-19 