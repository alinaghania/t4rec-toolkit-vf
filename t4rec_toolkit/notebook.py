# === T4REC XLNET AVEC SEULEMENT DES FEATURES CAT√âGORIELLES (PLUS STABLE) ===

print("üöÄ T4REC XLNET - FEATURES CAT√âGORIELLES UNIQUEMENT - STABLE")
print("=" * 65)

import torch
import transformers4rec.torch as tr
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

try:
    # Configuration T4Rec SIMPLIFI√âE pour version 23.04.00
    CONFIG = {
        'd_model': 128,
        'n_head': 4,
        'n_layer': 2,
        'max_sequence_length': 15,
        'mem_len': 30,
        'dropout': 0.1,
        'hidden_size': 128,  # OBLIGATOIRE pour MaskSequence
        'vocab_size': 100    # OBLIGATOIRE pour XLNetConfig
    }

    # 1. Cr√©er le sch√©ma T4Rec SANS features continues (plus stable)
    print("üìã Sch√©ma T4Rec avec SEULEMENT des features cat√©gorielles...")
    
    from merlin.schema import Schema, ColumnSchema, Tags
    
    # Sch√©ma SIMPLIFI√â avec SEULEMENT des features cat√©gorielles
    columns = [
        # Item ID - OBLIGATOIREMENT en premier
        ColumnSchema(
            "item_id",
            tags=[Tags.CATEGORICAL, Tags.ITEM, Tags.ID],
            dtype=np.int32,
            properties={"domain": {"min": 1, "max": 100}}
        ),
        # User category
        ColumnSchema(
            "user_category",
            tags=[Tags.CATEGORICAL, Tags.USER],
            dtype=np.int32, 
            properties={"domain": {"min": 1, "max": 20}}
        )
        # PAS de feature continue pour √©viter les probl√®mes
    ]
    
    schema = Schema(columns)
    print(f"‚úÖ Sch√©ma simplifi√©: item_id + user_category SEULEMENT")
    
    # 2. Donn√©es optimis√©es pour T4Rec (SEULEMENT cat√©gorielles)
    print("\nüìä Donn√©es pour T4Rec (cat√©gorielles uniquement)...")
    
    # Prendre le nombre d'√©chantillons
    n_samples = len(next(iter(tabular_data.values())))
    print(f"√âchantillons: {n_samples}")
    
    # Item IDs : cycle sur une petite plage
    item_ids = np.arange(n_samples) % 99 + 1  # IDs de 1 √† 99
    
    # User categories : bas√© sur vos features dummy
    categorical_names = [name for name in tabular_data.keys() if 'dummy' in name]
    if categorical_names:
        base_cat = np.array(tabular_data[categorical_names[0]])
        user_categories = (base_cat % 19) + 1  # 1 √† 19
    else:
        user_categories = np.random.randint(1, 20, n_samples)
    
    # Dataset T4Rec SIMPLIFI√â (seulement cat√©gorielles)
    t4rec_data = {
        "item_id": item_ids.astype(np.int64),
        "user_category": user_categories.astype(np.int64)
        # PAS de continuous_feature
    }
    
    print(f"‚úÖ item_id: {len(np.unique(item_ids))} uniques ({item_ids.min()}-{item_ids.max()})")
    print(f"‚úÖ user_category: {len(np.unique(user_categories))} uniques")
    
    # 3. Cr√©er le module d'entr√©e SANS continuous_projection
    print("\nüèóÔ∏è Module d'entr√©e (seulement cat√©gorielles)...")
    
    # Module SIMPLIFI√â sans continuous_projection
    input_module = tr.TabularSequenceFeatures.from_schema(
        schema=schema,
        max_sequence_length=CONFIG['max_sequence_length'],
        # PAS de continuous_projection car pas de features continues
        aggregation="concat",
        masking=None,  # PAS de masking initially
        automatic_build=False  # OK maintenant car pas de continuous_projection
    )
    
    print(f"‚úÖ Module cr√©√©: {type(input_module).__name__}")
    
    # 4. Ajouter le masking manuellement
    print("\nüé≠ Configuration du masking...")
    from transformers4rec.torch.masking import MaskSequence
    
    try:
        masking_module = MaskSequence(
            schema=schema.select_by_tag(Tags.ITEM),
            hidden_size=CONFIG['hidden_size'],  # PARAM√àTRE OBLIGATOIRE
            max_sequence_length=CONFIG['max_sequence_length'],
            masking_prob=0.2,
            padding_idx=0
        )
        
        input_module.masking = masking_module
        print(f"‚úÖ Masking assign√©: {type(masking_module).__name__}")
        
    except Exception as masking_error:
        print(f"‚ö†Ô∏è Erreur masking: {masking_error}")
        input_module.masking = None
        print("‚ö†Ô∏è Continuons sans masking pour le test")
    
    # 5. Configuration XLNet COMPL√àTE
    print("\n‚öôÔ∏è Config XLNet...")
    
    xlnet_config = tr.XLNetConfig.build(
        d_model=CONFIG['d_model'],
        n_head=CONFIG['n_head'], 
        n_layer=CONFIG['n_layer'],
        total_seq_length=CONFIG['max_sequence_length'],
        mem_len=CONFIG['mem_len'],
        dropout=CONFIG['dropout'],
        pad_token_id=0,
        vocab_size=CONFIG['vocab_size'],
        attn_type='bi',
        initializer_range=0.02,
        hidden_act='gelu',
        layer_norm_eps=1e-12
    )
    
    print(f"‚úÖ XLNet: {CONFIG['d_model']}d, {CONFIG['n_head']}h, {CONFIG['n_layer']}l")
    
    # 6. Construire le mod√®le
    print("\nüöÄ Mod√®le T4Rec...")
    
    # Corps du mod√®le
    body = tr.SequentialBlock(
        input_module,
        tr.TransformerBlock(xlnet_config, masking=input_module.masking)
    )
    
    # M√©triques
    from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt
    
    # T√™te de pr√©diction
    head = tr.Head(
        body,
        tr.NextItemPredictionTask(
            weight_tying=True,
            hf_format=True,
            metrics=[
                NDCGAt(top_ks=[5, 10], labels_onehot=True),
                RecallAt(top_ks=[5, 10], labels_onehot=True)
            ],
            loss_function="cross_entropy"
        ),
        inputs=input_module
    )
    
    # Mod√®le complet
    model = tr.Model(head)
    
    print(f"‚úÖ Mod√®le T4Rec cr√©√©!")
    print(f"üìà Param√®tres: {sum(p.numel() for p in model.parameters()):,}")
    
    # 7. Test rapide du mod√®le
    print("\nüß™ Test du mod√®le...")
    
    # Cr√©er un batch test avec s√©quences
    batch_size = 16
    sequenced_batch = {}
    
    for key, data in t4rec_data.items():
        tensor = torch.tensor(data[:batch_size], dtype=torch.long)
        # Cr√©er des s√©quences en r√©p√©tant
        seq_tensor = tensor.unsqueeze(1).expand(-1, CONFIG['max_sequence_length'])
        sequenced_batch[key] = seq_tensor
    
    print(f"Batch de test: {[(k, v.shape) for k, v in sequenced_batch.items()]}")
    
    model.eval()
    with torch.no_grad():
        try:
            output = model(sequenced_batch)
            print(f"‚úÖ Test r√©ussi: output shape={output.shape}")
            test_success = True
        except Exception as test_error:
            print(f"‚ùå Test √©chou√©: {test_error}")
            print(f"Type d'erreur: {type(test_error).__name__}")
            import traceback
            traceback.print_exc()
            test_success = False
    
    if test_success:
        # 8. Entra√Ænement
        print("\nüéØ Entra√Ænement...")
        
        # Pr√©parer toutes les donn√©es avec s√©quences
        full_dataset = {}
        
        for key, data in t4rec_data.items():
            tensor = torch.tensor(data, dtype=torch.long)
            
            # Cr√©er des s√©quences plus r√©alistes
            sequences = []
            for i in range(len(tensor)):
                # Cr√©er une s√©quence avec les √©l√©ments pr√©c√©dents
                base_val = tensor[i]
                seq = torch.full((CONFIG['max_sequence_length'],), base_val, dtype=tensor.dtype)
                
                # Ajouter quelques variations pour plus de r√©alisme
                if i > 0:
                    # Prendre les √©l√©ments pr√©c√©dents
                    start_idx = max(0, i - CONFIG['max_sequence_length'] + 1)
                    prev_seq = tensor[start_idx:i]
                    if len(prev_seq) > 0:
                        seq[-len(prev_seq):] = prev_seq
                    seq[-1] = base_val  # √âl√©ment courant en dernier
                
                sequences.append(seq)
            
            full_dataset[key] = torch.stack(sequences)
        
        print(f"Dataset final: {[(k, v.shape) for k, v in full_dataset.items()]}")
        
        # Pr√©parer les labels
        y = df['souscription_produit_1m'].values
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)
        y_tensor = torch.tensor(y_encoded, dtype=torch.long)
        
        # Split train/validation
        n_samples_total = len(y_tensor)
        n_train = int(0.8 * n_samples_total)
        
        train_data = {k: v[:n_train] for k, v in full_dataset.items()}
        val_data = {k: v[n_train:] for k, v in full_dataset.items()}
        y_train = y_tensor[:n_train]
        y_val = y_tensor[n_train:]
        
        print(f"‚úÖ Train: {n_train}, Val: {len(y_val)}")
        
        # Configuration d'entra√Ænement
        from torch.optim import AdamW
        from torch.nn import CrossEntropyLoss
        
        optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
        criterion = CrossEntropyLoss()
        
        # Device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        print(f"Device utilis√©: {device}")
        
        # Boucle d'entra√Ænement
        num_epochs = 3
        batch_size_train = 8
        
        for epoch in range(num_epochs):
            model.train()
            total_loss = 0
            n_batches = 0
            
            # Mini-batches d'entra√Ænement
            for i in range(0, len(y_train), batch_size_train):
                end_idx = min(i + batch_size_train, len(y_train))
                
                batch_data = {
                    k: v[i:end_idx].to(device) 
                    for k, v in train_data.items()
                }
                batch_targets = y_train[i:end_idx].to(device)
                
                optimizer.zero_grad()
                try:
                    outputs = model(batch_data)
                    loss = criterion(outputs, batch_targets)
                    loss.backward()
                    
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    
                    total_loss += loss.item()
                    n_batches += 1
                    
                except Exception as batch_error:
                    print(f"‚ö†Ô∏è Erreur batch {i//batch_size_train + 1}: {batch_error}")
                    continue
            
            avg_loss = total_loss / max(n_batches, 1)
            print(f"√âpoque {epoch + 1}/{num_epochs}: Loss = {avg_loss:.4f}")
        
        # √âvaluation finale
        print("\nüìä √âvaluation finale...")
        model.eval()
        correct = 0
        total_eval = 0
        
        with torch.no_grad():
            for i in range(0, len(y_val), batch_size_train):
                end_idx = min(i + batch_size_train, len(y_val))
                
                batch_data = {
                    k: v[i:end_idx].to(device)
                    for k, v in val_data.items()
                }
                batch_targets = y_val[i:end_idx].to(device)
                
                try:
                    outputs = model(batch_data)
                    predictions = torch.argmax(outputs, dim=1)
                    correct += (predictions == batch_targets).sum().item()
                    total_eval += batch_targets.size(0)
                except:
                    continue
        
        accuracy = correct / max(total_eval, 1)
        print(f"‚úÖ Pr√©cision finale: {accuracy:.2%}")
        
        # Sauvegarde
        save_dict = {
            'model_state_dict': model.state_dict(),
            'config': CONFIG,
            'schema': schema,
            'label_encoder': label_encoder,
            'accuracy': accuracy,
            'transformers4rec_version': '23.04.00',
            'architecture': 'categorical_only_stable'
        }
        
        torch.save(save_dict, 't4rec_xlnet_categorical_only_success.pth')
        print("‚úÖ Mod√®le sauvegard√©: t4rec_xlnet_categorical_only_success.pth")
        print("üéâ SUCC√àS T4REC XLNET CAT√âGORIELLES UNIQUEMENT!")
    
    else:
        print("‚ùå √âchec du test - pas d'entra√Ænement")

except Exception as e:
    print(f"‚ùå ERREUR G√âN√âRALE: {e}")
    import traceback
    traceback.print_exc()
    
    print(f"\nüîç DEBUG:")
    if 'schema' in locals():
        try:
            print(f"Sch√©ma: {len(schema)} colonnes")
            if hasattr(schema, 'column_schemas'):
                column_names = [col.name for col in schema.column_schemas]
                print(f"Colonnes: {column_names}")
        except:
            print("Impossible d'acc√©der aux colonnes du sch√©ma")
