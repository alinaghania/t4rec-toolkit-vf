
# t4rec_toolkit/pipeline_core.py
# -*- coding: utf-8 -*-
"""
PIPELINE HYBRIDE T4Rec + PyTorch avec DIMENSION TEMPORELLE R√âELLE
------------------------------------------------------------------
Ce module :
  1) charge les donn√©es (profil + events) depuis Dataiku,
  2) construit de VRAIES s√©quences temporelles par client (12-24 mois),
  3) pr√©pare les features (cat√©gorielles / s√©quentielles) pour T4Rec,
  4) entra√Æne un Transformer PyTorch sur ces s√©quences,
  5) calcule les m√©triques + sauvegarde datasets (features/pred/metrics/model).

Points cl√©s :
  - Compatible T4Rec 23.04.00 (on n'utilise que les embeddings)
  - Exploite enfin la dimension temporelle (le vrai "+")
  - Regroupement des classes ultra-rares ‚Üí "AUTRES_PRODUITS" (log explicite)
"""

from __future__ import annotations
import logging, time, json, math
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn

# ============== T4Rec (embeddings uniquement) =====================
try:
    import transformers4rec.torch as tr
    from transformers4rec.torch.features.embedding import (
        EmbeddingFeatures,
        FeatureConfig,
        TableConfig,
    )
    from transformers4rec.torch.features.sequence import SequenceEmbeddingFeatures
    _HAS_T4REC = True
    print("T4Rec import OK")
except Exception as e:
    _HAS_T4REC = False
    raise ImportError("T4Rec requis. Installez: pip install transformers4rec[torch]")

# ============== Dataiku (optionnel) ================================
try:
    import dataiku
    _HAS_DATAIKU = True
except Exception:
    _HAS_DATAIKU = False

# ============== TQDM (optionnel) ==================================
try:
    from tqdm.auto import tqdm
    _HAS_TQDM = True
except Exception:
    _HAS_TQDM = False

# ============== Transfos locales ==================================
from .transformers.sequence_transformer import SequenceTransformer  # corrig√©
from .transformers.categorical_transformer import CategoricalTransformer  # corrig√©
from .utils.sequence_builder import SequenceBuilder, SequenceBuilderConfig  # nouveau

# ============== Sklearn (label encoding / metrics) =================
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_recall_fscore_support


# ------------------------------------------------------------------
# 1) CONFIGS & HELPERS
# ------------------------------------------------------------------

@dataclass
class T4RecConfig:
    """Config du mod√®le (dimensions principales)"""
    d_model: int = 256
    n_heads: int = 8
    n_layers: int = 4
    dropout: float = 0.1
    max_sequence_length: int = 24
    vocab_size: int = 2000


def blank_config() -> Dict[str, Any]:
    """
    Gabarit de configuration.
    NOTE: nouvelles cl√©s "events_*" pour activer la dimension temporelle.
    """
    return {
        "data": {
            "dataset_name": "",            # table statique (profil) - optionnelle
            "events_dataset": "",          # table √©v√®nements (obligatoire pour les s√©quences)
            "sample_size": 100_000,        # nb max clients
            "limit": None,
            "chunk_size": 50_000,
            "partitions": None,
            "temporal_split": None,
            # Colonnes (events)
            "client_id_col": "CLIENT_ID",
            "event_time_col": "DATE_EVENT",       # doit √™tre parsable datetime
            "product_col": "PRODUCT_CODE",        # identifiant produit
            # Optionnels : d'autres attributs d'√©v√©nement (canal, montant, etc.)
            "event_extra_cols": [],               # ex: ["CANAL", "MONTANT"]
            # Colonnes profil (si dataset profil est fourni)
            "profile_join_key": "CLIENT_ID",
            "profile_categorical_cols": [],       # ex: ["SEGMENT", "REGION"]
            "profile_sequence_cols": [],          # ex: ["AGE", "REVENU"]
        },
        "sequence": {
            # Fen√™tre temporelle / sampling
            "months_lookback": 24,         # longueur de s√©quence (derniers n mois)
            "min_events_per_client": 1,    # garde les clients avec au moins 1 event
            "time_granularity": "M",       # "M" = mois, "W" = semaine (support de base)
            "target_horizon": 1,           # pr√©dire l'item du mois suivant (horizon=1)
            "pad_value": 0,                # padding index pour les s√©quences
            "build_target_from_events": True,  # True: label=produit du "mois cible"
        },
        "features": {
            "sequence_cols": [],           # (rempli automatiquement pour events)
            "categorical_cols": [],        # (profil ou events extra)
            "target_col": "TARGET_PRODUCT",# label final (si pas build auto)
            "exclude_target_values": ["aucune_souscription"],
            "merge_rare_threshold": 200,   # <200 exemples ‚Üí "AUTRES_PRODUITS"
            "other_class_name": "AUTRES_PRODUITS",
        },
        "model": {
            "d_model": 512,
            "n_heads": 8,
            "n_layers": 4,
            "dropout": 0.1,
            "max_sequence_length": 24,
            "vocab_size": 2000,            # base pour embeddings
        },
        "training": {
            "batch_size": 64,
            "num_epochs": 20,
            "learning_rate": 5e-4,
            "weight_decay": 1e-4,
            "val_split": 0.2,
            "class_weighting": True,       # pond√©rer CrossEntropy selon fr√©quences
            "gradient_clip": 1.0,
            "optimizer": "adamw",
        },
        "outputs": {
            "features_dataset": "T4REC_FEATURES",
            "predictions_dataset": "T4REC_PREDICTIONS",
            "metrics_dataset": "T4REC_METRICS",
            "model_artifacts_dataset": "T4REC_MODEL",
            "local_dir": "output",
        },
        "runtime": {
            "verbose": True,
            "progress": True,
            "seed": 42,
        },
    }


def _setup_logging(verbose: bool) -> None:
    """Logging simple."""
    level = logging.INFO if verbose else logging.WARNING
    logging.basicConfig(level=level, format="%(asctime)s - %(levelname)s - %(message)s")


# ------------------------------------------------------------------
# 2) CHARGEMENT & PR√âPARATION DONN√âES
# ------------------------------------------------------------------

def _load_events_df(cfg: Dict[str, Any]) -> pd.DataFrame:
    """
    Charge le dataset d'√©v√©nements (obligatoire pour la dimension temporelle).
    Le dataset doit contenir au minimum :
      - client_id_col
      - event_time_col (datetime-like)
      - product_col (id produit)
    """
    if not _HAS_DATAIKU:
        raise RuntimeError("Dataiku n√©cessaire pour charger les datasets.")

    logger = logging.getLogger(__name__)
    dcfg = cfg["data"]
    ds_name = dcfg["events_dataset"]
    if not ds_name:
        raise ValueError("data.events_dataset est requis (s√©quences temporelles).")

    ds = dataiku.Dataset(ds_name)
    df = ds.get_dataframe(limit=dcfg.get("limit"))
    logger.info(f"Events loaded: {ds_name} ‚Üí {df.shape}")

    # Renommer pour simplifier le code interne
    cid = dcfg["client_id_col"]
    tcol = dcfg["event_time_col"]
    pcol = dcfg["product_col"]
    if tcol not in df.columns or cid not in df.columns or pcol not in df.columns:
        raise ValueError(
            f"Colonnes requises manquantes dans events: "
            f"{cid}, {tcol}, {pcol}."
        )

    # Ensure datetime
    df[tcol] = pd.to_datetime(df[tcol], errors="coerce")
    df = df.dropna(subset=[tcol])  # retire lignes non dat√©es
    return df


def _load_profile_df_if_any(cfg: Dict[str, Any]) -> Optional[pd.DataFrame]:
    """
    Charge la table profil si "dataset_name" est fourni.
    (Optionnelle : utile pour features cat√©gorielles/statique par client)
    """
    if not _HAS_DATAIKU:
        return None

    logger = logging.getLogger(__name__)
    dcfg = cfg["data"]
    ds_name = dcfg.get("dataset_name")
    if not ds_name:
        return None

    ds = dataiku.Dataset(ds_name)
    df = ds.get_dataframe(limit=dcfg.get("limit"))
    logger.info(f"Profile loaded: {ds_name} ‚Üí {df.shape}")
    return df


def merge_rare_classes(series: pd.Series, min_count: int, other_name: str) -> Tuple[pd.Series, Dict[str, str]]:
    """
    Regroupe les classes rares (< min_count) en 'other_name'.
    Retourne la s√©rie transform√©e + le mapping classe_originale -> 'other_name'.
    """
    vc = series.value_counts()
    rare = vc[vc < min_count].index
    mapping = {str(x): other_name for x in rare}
    new_series = series.astype(str).where(~series.astype(str).isin(rare), other_name)
    return new_series, mapping


# ------------------------------------------------------------------
# 3) MOD√àLE HYBRIDE
# ------------------------------------------------------------------

class T4RecTemporalModel(nn.Module):
    """
    Mod√®le hybride :
      - Embeddings T4Rec pour les features (s√©quence item_id + extras)
      - TransformerEncoder PyTorch qui lit toute la s√©quence
      - Head de pr√©diction du prochain produit (multi-classes)
    """

    def __init__(
        self,
        embedding_module,           # SequenceEmbeddingFeatures T4Rec
        d_model: int,
        n_heads: int,
        n_layers: int,
        dropout: float,
        n_classes: int,
        max_seq_len: int,
        proj_in_dim: Optional[int] = None,
    ):
        super().__init__()
        self.t4rec_embeddings = embedding_module

        # Si la concat T4Rec ne fait pas "pile" d_model, on projette
        self.proj = None
        if proj_in_dim is not None and proj_in_dim != d_model:
            self.proj = nn.Linear(proj_in_dim, d_model)

        # Positional encoding simple (appris)
        self.positional = nn.Parameter(torch.randn(1, max_seq_len, d_model) * 0.02)

        # Transformer
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            activation="gelu",
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)

        # Head
        self.head = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, n_classes),
        )

        self.d_model = d_model
        self.max_seq_len = max_seq_len

    def forward(self, batch_inputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        batch_inputs = dict de tenseurs Long [B, T] pour toutes les features
        """
        # 1) Embeddings T4Rec (concat par feature ‚Üí [B, T, F_concat])
        x = self.t4rec_embeddings(batch_inputs)  # [B, T, F_concat]

        # 2) Projection √©ventuelle pour matcher d_model
        if self.proj is not None:
            x = self.proj(x)  # [B, T, d_model]

        # 3) + positional
        T = x.shape[1]
        pos = self.positional[:, :T, :]
        x = x + pos

        # 4) Transformer
        x = self.encoder(x)  # [B, T, d_model]

        # 5) On prend le dernier pas de temps comme "r√©sum√©" (next-item)
        out = x[:, -1, :]  # [B, d_model]

        # 6) Logits multi-classes
        logits = self.head(out)  # [B, n_classes]
        return logits


# ------------------------------------------------------------------
# 4) CR√âATION EMBEDDINGS T4REC
# ------------------------------------------------------------------

def _build_t4rec_embedding_module(
    feature_dims: Dict[str, int],
    d_model: int,
    max_seq_len: int,
) -> Tuple[SequenceEmbeddingFeatures, int]:
    """
    Construit un SequenceEmbeddingFeatures o√π chaque feature a sa TableConfig.
    Retourne (module, concat_dim).
    - feature_dims: dict {feature_name: vocab_size}
      Ex: {"item_id": 5000, "canal": 10}
    - Chaque table a une dimension d_model // nb_features (simple & robuste)
    """
    assert _HAS_T4REC
    n_feats = len(feature_dims)
    per_dim = max(8, d_model // max(1, n_feats))  # au moins 8

    feature_cfgs = {}
    for feat, vocab in feature_dims.items():
        tbl = TableConfig(vocabulary_size=vocab, dim=per_dim, name=f"{feat}_table")
        feature_cfgs[feat] = FeatureConfig(
            table=tbl, max_sequence_length=max_seq_len, name=feat
        )

    # item_id = 1√®re feature par convention (obligatoire pour T4Rec)
    item_id = list(feature_dims.keys())[0]
    emb = SequenceEmbeddingFeatures(feature_config=feature_cfgs, item_id=item_id, aggregation="concat")
    concat_dim = per_dim * n_feats
    return emb, concat_dim


# ------------------------------------------------------------------
# 5) ENTRA√éNEMENT
# ------------------------------------------------------------------

def run_training(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Pipeline complet :
      - construction des s√©quences temporelles (SequenceBuilder),
      - encodage des features (cat/seq),
      - embeddings T4Rec + Transformer,
      - training/validation,
      - m√©triques + sauvegardes.
    """
    t0 = time.time()
    _setup_logging(config["runtime"]["verbose"])
    logger = logging.getLogger(__name__)

    # Seeds
    seed = config["runtime"].get("seed")
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)

    # 1) Charger donn√©es
    events_df = _load_events_df(config)
    profile_df = _load_profile_df_if_any(config)

    # 2) Construire s√©quences temporelles
    sb_cfg = SequenceBuilderConfig(
        client_id_col=config["data"]["client_id_col"],
        time_col=config["data"]["event_time_col"],
        item_col=config["data"]["product_col"],
        extra_event_cols=config["data"].get("event_extra_cols", []),
        months_lookback=config["sequence"]["months_lookback"],
        time_granularity=config["sequence"]["time_granularity"],
        min_events_per_client=config["sequence"]["min_events_per_client"],
        target_horizon=config["sequence"]["target_horizon"],
        pad_value=config["sequence"]["pad_value"],
        build_target_from_events=config["sequence"]["build_target_from_events"],
    )
    builder = SequenceBuilder(sb_cfg)
    seq_pack = builder.fit_transform(events_df)
    # seq_pack contient :
    #  - "X_seq": dict {feature_name: np.array [N_clients, T] d'ids}
    #  - "y": np.array (cible item √† T+horizon) si build_target_from_events=True
    #  - "client_ids": ordre des clients
    #  - "vocab_sizes": dict {feature_name: vocab_size}
    logger.info(f"[SequenceBuilder] X_seq keys={list(seq_pack['X_seq'].keys())}, shape={next(iter(seq_pack['X_seq'].values())).shape}")

    # 3) Cr√©er/ajouter features profil (facultatif)
    #    ‚Üí on traite les colonnes profil (cat√©gorielles / "sequence_cols" mono-pas)
    X_cat = {}
    X_seq_extra = {}
    if profile_df is not None:
        # Joindre sur client_id
        pid = config["data"]["profile_join_key"]
        prof = profile_df.set_index(pid).reindex(seq_pack["client_ids"]).reset_index()
        prof.columns = [pid] + list(prof.columns[1:])

        # CATEGORIELLES profil ‚Üí encodage entiers
        cat_cols = config["data"].get("profile_categorical_cols", [])
        if len(cat_cols) > 0:
            cat_tr = CategoricalTransformer(handle_unknown="encode", unknown_value=1, name="ProfileCategorical")
            cat_tr.fit(prof, cat_cols)
            cat_res = cat_tr.transform(prof)
            # Chaque col devient <col>_encoded ‚Üí vecteur [N_clients]
            for k, arr in cat_res.data.items():
                # Pour compat T4Rec (qui attend [B,T]), on r√©p√®te sur T et on mettra petite table
                X_cat[k.replace("_encoded", "")] = arr.astype(np.int64)

        # SEQ "mono-pas" profil (ex: √¢ge) ‚Üí on en fait une "s√©quence plate" (r√©p√©t√©e sur T)
        seq_cols = config["data"].get("profile_sequence_cols", [])
        if len(seq_cols) > 0:
            seq_tr = SequenceTransformer(name="ProfileSeq")
            seq_tr.fit(prof, seq_cols)
            seq_res = seq_tr.transform(prof)
            for k, arr in seq_res.data.items():
                # arr est float [N], on va le discr√©tiser pour embeddings ‚Üí indices int dans [0, vocab-1]
                # on choisit un vocab "profil" petit (ex 200) et un simple scale
                vocab_p = 200
                arr01 = np.clip(arr, 0.0, 1.0)
                X_seq_extra[k.replace("_seq", "")] = (arr01 * (vocab_p - 1)).astype(np.int64)

    # 4) Cible (y)
    if config["sequence"]["build_target_from_events"]:
        y_raw = seq_pack["y"]  # items (str ou id) √† T+h
        y_series = pd.Series(y_raw).astype(str)
    else:
        # Alternative : target fournie dans une table (moins recommand√© ici)
        raise ValueError("Pour la version temporelle, utilisez build_target_from_events=True.")

    # 5) Regroupement classes rares
    min_count = config["features"]["merge_rare_threshold"]
    other_name = config["features"]["other_class_name"]
    y_merged, rare_map = merge_rare_classes(y_series, min_count=min_count, other_name=other_name)

    if len(rare_map) > 0:
        logger.info("=== Regroupement classes rares ‚Üí 'AUTRES_PRODUITS' ===")
        # On logue un r√©sum√© compact
        tops = list(rare_map.keys())[:20]
        logger.info(f"  {len(rare_map)} classes fusionn√©es. Exemples: {tops} ‚Üí {other_name}")
        # On sauvegarde aussi le mapping complet en JSON dans les artifacts plus tard
    else:
        logger.info("Aucune classe fusionn√©e (pas de classe < seuil).")

    # 6) Encodage cible (LabelEncoder)
    encoder = LabelEncoder()
    y = encoder.fit_transform(y_merged.values)
    n_classes = len(encoder.classes_)
    logger.info(f"Cible: {n_classes} classes apr√®s fusion √©ventuelle.")

    # 7) Pr√©parer les features pour T4Rec ‚Üí tout doit √™tre des IDs entiers dans [0, vocab-1]
    #   7.1) features s√©quentielles principales (events) d√©j√† en IDs int via SequenceBuilder
    #        (ex: "item_id" + autres extras d‚Äô√©v√©nements optionnels)
    X_seq_dict = {}
    for feat, mat in seq_pack["X_seq"].items():  # mat [N, T] int64
        X_seq_dict[feat] = torch.tensor(mat, dtype=torch.long)

    #   7.2) features profil "seq_extra" (r√©p√©ter par T)
    for feat, vec in X_seq_extra.items():  # vec [N]
        repeated = np.repeat(vec[:, None], repeats=X_seq_dict[list(X_seq_dict.keys())[0]].shape[1], axis=1)
        X_seq_dict[feat] = torch.tensor(repeated, dtype=torch.long)

    #   7.3) features profil cat√©gorielles (non s√©quentielles) ‚Üí r√©p√©ter sur T
    for feat, vec in X_cat.items():
        repeated = np.repeat(vec[:, None], repeats=X_seq_dict[list(X_seq_dict.keys())[0]].shape[1], axis=1)
        X_seq_dict[feat] = torch.tensor(repeated.astype(np.int64), dtype=torch.long)

    # 8) Vocab sizes (pour embeddings)
    vocab_sizes = dict(seq_pack["vocab_sizes"])  # events
    # ajouter profils "seq_extra"
    for feat in X_seq_extra.keys():
        vocab_sizes[feat] = vocab_sizes.get(feat, 200)
    # ajouter profils cat√©goriels
    for feat in X_cat.keys():
        # approx : 1 + max val observ√©e (sinon 50 par d√©faut)
        vmax = int(np.max(X_cat[feat])) if len(X_cat[feat]) > 0 else 1
        vocab_sizes[feat] = max(vmax + 1, 10)

    # 9) Split train/val
    N = len(y)
    idx = np.arange(N)
    np.random.shuffle(idx)
    split = int(N * (1.0 - config["training"]["val_split"]))
    tr_idx, va_idx = idx[:split], idx[split:]

    def take_idx(d: Dict[str, torch.Tensor], ids: np.ndarray) -> Dict[str, torch.Tensor]:
        return {k: v[ids] for k, v in d.items()}

    Xtr = take_idx(X_seq_dict, tr_idx)
    Xva = take_idx(X_seq_dict, va_idx)
    ytr = torch.tensor(y[tr_idx], dtype=torch.long)
    yva = torch.tensor(y[va_idx], dtype=torch.long)

    # 10) Module d'embeddings T4Rec
    d_model = config["model"]["d_model"]
    max_T = config["model"]["max_sequence_length"]
    emb_mod, concat_dim = _build_t4rec_embedding_module(
        feature_dims=vocab_sizes,
        d_model=d_model,
        max_seq_len=max_T,
    )

    # 11) Mod√®le
    model = T4RecTemporalModel(
        embedding_module=emb_mod,
        d_model=d_model,
        n_heads=config["model"]["n_heads"],
        n_layers=config["model"]["n_layers"],
        dropout=config["model"]["dropout"],
        n_classes=n_classes,
        max_seq_len=max_T,
        proj_in_dim=concat_dim,
    )
    logger.info(f"Model params: {sum(p.numel() for p in model.parameters()):,}")

    # 12) Optim / Loss
    if config["training"]["optimizer"].lower() == "adamw":
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config["training"]["learning_rate"],
            weight_decay=config["training"]["weight_decay"],
        )
    else:
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config["training"]["learning_rate"],
            weight_decay=config["training"]["weight_decay"],
        )

    # Class weights (optionnel)
    if config["training"]["class_weighting"]:
        counts = np.bincount(ytr.numpy(), minlength=n_classes).astype(np.float32)
        inv = 1.0 / np.clip(counts, 1.0, None)
        weights = inv / inv.sum() * n_classes
        class_w = torch.tensor(weights, dtype=torch.float32)
        loss_fn = nn.CrossEntropyLoss(weight=class_w)
    else:
        loss_fn = nn.CrossEntropyLoss()

    # 13) Training loop (full-batch pour simplicit√© ; Dataiku ‚Üí mini-batches possible)
    num_epochs = config["training"]["num_epochs"]
    bs = config["training"]["batch_size"]
    grad_clip = config["training"].get("gradient_clip", None)

    def iterate_batches(X: Dict[str, torch.Tensor], y: torch.Tensor, batch_size: int):
        N = len(y)
        for s in range(0, N, batch_size):
            e = min(s + batch_size, N)
            yield {k: v[s:e] for k, v in X.items()}, y[s:e]

    pbar = tqdm(range(num_epochs), desc="Training") if (_HAS_TQDM and config["runtime"]["progress"]) else range(num_epochs)
    history = []

    for epoch in pbar:
        model.train()
        tr_loss = 0.0; n_steps = 0
        for xb, yb in iterate_batches(Xtr, ytr, bs):
            optimizer.zero_grad()
            logits = model(xb)
            loss = loss_fn(logits, yb)
            loss.backward()
            if grad_clip:
                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            optimizer.step()
            tr_loss += loss.item(); n_steps += 1
        tr_loss /= max(1, n_steps)

        # Validation
        model.eval()
        with torch.no_grad():
            # full val (simple)
            logits = model(Xva)
            va_loss = loss_fn(logits, yva).item()
            preds = torch.argmax(logits, dim=1)
            acc = (preds == yva).float().mean().item()

        history.append({"epoch": epoch + 1, "train_loss": tr_loss, "val_loss": va_loss, "val_accuracy": acc})
        if _HAS_TQDM and config["runtime"]["progress"]:
            pbar.set_postfix({"loss": f"{tr_loss:.4f}", "val_acc": f"{acc:.4f}"})
        logger.info(f"Epoch {epoch+1}/{num_epochs} | loss={tr_loss:.4f} | val_loss={va_loss:.4f} | val_acc={acc:.4f}")

    # 14) Final metrics
    model.eval()
    with torch.no_grad():
        logits = model(Xva)
        preds = torch.argmax(logits, dim=1)
        acc = (preds == yva).float().mean().item()
        precision, recall, f1, _ = precision_recall_fscore_support(
            yva.numpy(), preds.numpy(), average="weighted", zero_division=0
        )

    # 15) Top-K (format coll√®gue)
    prediction_scores = logits.cpu().numpy()
    true_labels = yva.cpu().numpy()

    # mapping inverse enc -> classe (str)
    inverse_target_mapping = {i: cls for i, cls in enumerate(encoder.classes_)}

    # On r√©utilise tes helpers (inchang√©s)
    topk_metrics, _ignored = evaluate_topk_metrics_nbo(
        predictions=prediction_scores,
        targets=true_labels,
        inverse_target_mapping=inverse_target_mapping,
        k_values=[1, 3, 5, 10],
    )

    # 16) Sauvegardes Dataiku (best effort)
    saved = {}
    if _HAS_DATAIKU:
        try:
            # PREDICTIONS (client, top1..top5)
            topk = 5
            pred_list = []
            for i, (score, ytrue) in enumerate(zip(prediction_scores, true_labels)):
                top_idx = np.argsort(score)[-topk:][::-1]
                row = {
                    "client_id": int(seq_pack["client_ids"][va_idx[i]]),
                    "true_label": str(inverse_target_mapping.get(int(ytrue), "UNK")),
                    "pred_top1": str(inverse_target_mapping.get(int(top_idx[0]), "UNK")),
                    "pred_top1_score": float(score[top_idx[0]]),
                }
                # facultatif : top2..top5
                for k in range(1, topk):
                    ki = top_idx[k]
                    row[f"pred_top{k+1}"] = str(inverse_target_mapping.get(int(ki), "UNK"))
                    row[f"pred_top{k+1}_score"] = float(score[ki])
                pred_list.append(row)

            pred_df = pd.DataFrame(pred_list)
            if config["outputs"].get("predictions_dataset"):
                dataiku.Dataset(config["outputs"]["predictions_dataset"]).write_with_schema(pred_df)
                saved["predictions"] = config["outputs"]["predictions_dataset"]

            # METRICS
            metrics_rows = []
            metrics_rows += [
                {"metric_name": "accuracy", "metric_value": float(acc), "metric_type": "standard", "dataset_split": "validation"},
                {"metric_name": "precision", "metric_value": float(precision), "metric_type": "standard", "dataset_split": "validation"},
                {"metric_name": "recall", "metric_value": float(recall), "metric_type": "standard", "dataset_split": "validation"},
                {"metric_name": "f1", "metric_value": float(f1), "metric_type": "standard", "dataset_split": "validation"},
            ]
            # top-k NBO
            for k, d in topk_metrics.items():
                metrics_rows += [
                    {"metric_name": "Precision@K", "metric_value": float(d.get("Precision@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "Recall@K", "metric_value": float(d.get("Recall@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "F1@K", "metric_value": float(d.get("F1@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "NDCG@K", "metric_value": float(d.get("NDCG@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "MAP", "metric_value": float(d.get("MAP", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "HitRate@K", "metric_value": float(d.get("HitRate@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "Coverage@K", "metric_value": float(d.get("Coverage@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "Clients_evaluated", "metric_value": float(d.get("Clients_evaluated", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                ]
            met_df = pd.DataFrame(metrics_rows)
            if config["outputs"].get("metrics_dataset"):
                dataiku.Dataset(config["outputs"]["metrics_dataset"]).write_with_schema(met_df)
                saved["metrics"] = config["outputs"]["metrics_dataset"]

            # ARTIFACTS (inclure mapping rarit√©)
            artifacts = pd.DataFrame({
                "artifact_name": ["model_config", "sequence_builder_config", "rare_class_mapping_json"],
                "artifact_value": [
                    json.dumps(config["model"]),
                    json.dumps(sb_cfg.__dict__),
                    json.dumps(rare_map, ensure_ascii=False),
                ],
                "timestamp": [pd.Timestamp.now()]*3,
            })
            if config["outputs"].get("model_artifacts_dataset"):
                dataiku.Dataset(config["outputs"]["model_artifacts_dataset"]).write_with_schema(artifacts)
                saved["model_artifacts"] = config["outputs"]["model_artifacts_dataset"]

        except Exception as e:
            logger.warning(f"Sauvegarde Dataiku √©chou√©e (non bloquant): {e}")

    # 17) Retour
    out = {
        "metrics": {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1},
        "predictions": {
            "raw_outputs": prediction_scores,
            "predicted_classes": preds.cpu().numpy(),
            "true_classes": true_labels,
        },
        "model_info": {
            "total_params": int(sum(p.numel() for p in model.parameters())),
            "architecture": f"Hybrid-T4Rec-Transformer {config['model']['n_layers']}L-{config['model']['n_heads']}H-{config['model']['d_model']}D",
        },
        "data_info": {
            "n_clients": int(len(y)),
            "seq_len": int(next(iter(X_seq_dict.values())).shape[1]),
            "features": list(X_seq_dict.keys()),
            "n_classes": int(n_classes),
        },
        "execution_time": float(time.time() - t0),
        "saved_datasets": saved,
    }
    return out


# ------------------------------------------------------------------
# 6) TOP-K (inchang√©, repris de ta version)
# ------------------------------------------------------------------

def compute_ranking_metrics_at_k(client_ids, labels, scores, products, k):
    """
    (copi√© / simplifi√© de ta version ; inchang√© dans l'esprit)
    """
    from sklearn.metrics import ndcg_score, average_precision_score
    from collections import defaultdict
    client_data = defaultdict(list)
    for cid, label, score, prod in zip(client_ids, labels, scores, products):
        client_data[cid].append((label, score, prod))

    ndcgs, aps, recalls, f1s = [], [], [], []
    hit_count = 0
    recommended_products = set()
    precision_topk_total = 0
    topk_count = 0
    valid_clients = 0

    for cid, items in client_data.items():
        y_true = np.array([l for l, _, _ in items], dtype=float)
        y_score = np.array([s for _, s, _ in items], dtype=float)
        y_prods = np.array([p for _, _, p in items])

        if y_true.sum() == 0 or len(y_true) < 2 or np.isnan(y_score).any():
            continue
        valid_clients += 1
        top_k_idx = np.argsort(y_score)[::-1][:k]
        y_topk = y_true[top_k_idx]
        p_topk = y_prods[top_k_idx]
        precision_topk_total += y_topk.sum()
        topk_count += k
        ndcgs.append(ndcg_score([y_true], [y_score], k=k))
        aps.append(average_precision_score(y_true, y_score))
        rec_k = y_topk.sum() / y_true.sum()
        recalls.append(rec_k)
        prec_k = y_topk.sum() / k
        if (prec_k + rec_k) > 0:
            f1s.append(2 * (prec_k * rec_k) / (prec_k + rec_k))
        if y_topk.sum() > 0:
            hit_count += 1
        recommended_products.update(p_topk)

    return {
        "Precision@K": precision_topk_total / topk_count if topk_count > 0 else 0.0,
        "Recall@K": float(np.mean(recalls)) if recalls else 0.0,
        "F1@K": float(np.mean(f1s)) if f1s else 0.0,
        "NDCG@K": float(np.mean(ndcgs)) if ndcgs else 0.0,
        "MAP": float(np.mean(aps)) if aps else 0.0,
        "HitRate@K": hit_count / valid_clients if valid_clients > 0 else 0.0,
        "Coverage@K": len(recommended_products) / len(set(products)) if len(products) > 0 else 0.0,
        "Clients_evaluated": valid_clients,
    }


def convert_predictions_to_nbo_format(predictions, targets, inverse_target_mapping):
    """
    Idem ta version.
    """
    client_ids_list, labels_list, scores_list, products_list = [], [], [], []
    n_samples, n_classes = predictions.shape
    for i in range(n_samples):
        pred = predictions[i]
        true_class = targets[i]
        probs = np.exp(pred) / np.sum(np.exp(pred))
        for j in range(n_classes):
            client_ids_list.append(i + 1)
            labels_list.append(1.0 if j == true_class else 0.0)
            scores_list.append(probs[j])
            products_list.append(inverse_target_mapping.get(j, f"UNK_{j}"))
    return (np.array(client_ids_list), np.array(labels_list), np.array(scores_list), np.array(products_list))


def evaluate_topk_metrics_nbo(predictions, targets, inverse_target_mapping, k_values=[1, 3, 5]):
    """
    Idem ta version : renvoie (metrics_dict, None)
    """
    client_ids, labels, scores, products = convert_predictions_to_nbo_format(
        predictions, targets, inverse_target_mapping
    )
    all_metrics = {}
    for k in k_values:
        all_metrics[k] = compute_ranking_metrics_at_k(client_ids, labels, scores, products, k)
    return all_metrics, None

# =====================================================================
# Validation de la configuration
# =====================================================================

def get_config_schema() -> Dict[str, Any]:
    """
    Sch√©ma minimal pour valider la config. 
    On contr√¥le surtout la pr√©sence/typage des cl√©s utilis√©es par le pipeline.
    """
    return {
        "data": {
            "events_dataset": {"type": str, "required": True},
            "client_id_col": {"type": str, "required": True},
            "event_time_col": {"type": str, "required": True},
            "product_col": {"type": str, "required": True},
            # optionnels
            "dataset_name": {"type": (str, type(None)), "required": False},
            "event_extra_cols": {"type": list, "required": False},
            "profile_categorical_cols": {"type": list, "required": False},
            "profile_sequence_cols": {"type": list, "required": False},
        },
        "sequence": {
            "months_lookback": {"type": int, "required": True},
            "time_granularity": {"type": str, "required": True},
            "min_events_per_client": {"type": int, "required": True},
            "target_horizon": {"type": int, "required": True},
            "pad_value": {"type": int, "required": True},
            "build_target_from_events": {"type": bool, "required": True},
        },
        "features": {
            "exclude_target_values": {"type": list, "required": False},
            "merge_rare_threshold": {"type": int, "required": True},
            "other_class_name": {"type": str, "required": True},
        },
        "model": {
            "d_model": {"type": int, "required": True},
            "n_heads": {"type": int, "required": True},
            "n_layers": {"type": int, "required": True},
            "dropout": {"type": float, "required": True},
            "max_sequence_length": {"type": int, "required": True},
            "vocab_size": {"type": int, "required": True},
        },
        "training": {
            "batch_size": {"type": int, "required": True},
            "num_epochs": {"type": int, "required": True},
            "learning_rate": {"type": float, "required": True},
            "weight_decay": {"type": float, "required": True},
            "val_split": {"type": float, "required": True},
            "class_weighting": {"type": bool, "required": True},
        },
        "outputs": {
            "features_dataset": {"type": (str, type(None)), "required": False},
            "predictions_dataset": {"type": (str, type(None)), "required": False},
            "metrics_dataset": {"type": (str, type(None)), "required": False},
            "model_artifacts_dataset": {"type": (str, type(None)), "required": False},
            "local_dir": {"type": str, "required": True},
        },
        "runtime": {
            "verbose": {"type": bool, "required": True},
            "progress": {"type": bool, "required": True},
            "seed": {"type": (int, type(None)), "required": True},
        },
    }


def validate_config(config: Dict[str, Any], strict: bool = False) -> List[str]:
    """
    Valide la config avant entra√Ænement.
    - V√©rifie la pr√©sence et le type de cl√©s essentielles
    - V√©rifie contraintes simples (ex: d_model % n_heads == 0)
    
    Retour:
        Liste d'erreurs (vide si tout est OK)
    """
    errors: List[str] = []
    schema = get_config_schema()

    # Helper pour naviguer dans le dict selon le sch√©ma
    def _check_block(block_name: str, block_schema: Dict[str, Any]):
        if block_name not in config:
            errors.append(f"Bloc '{block_name}' manquant dans la config")
            return
        block = config[block_name]
        for key, spec in block_schema.items():
            required = spec.get("required", False)
            expected_type = spec.get("type", object)
            if required and key not in block:
                errors.append(f"{block_name}.{key} est requis")
                continue
            if key in block and expected_type is not None:
                if not isinstance(block[key], expected_type):
                    errors.append(
                        f"{block_name}.{key} doit √™tre de type {expected_type} (actuel: {type(block[key])})"
                    )

    for section, sect_schema in schema.items():
        _check_block(section, sect_schema)

    # Contraintes sp√©cifiques mod√®le
    if "model" in config:
        m = config["model"]
        if isinstance(m.get("d_model"), int) and isinstance(m.get("n_heads"), int):
            if m["d_model"] % m["n_heads"] != 0:
                errors.append("model.d_model doit √™tre divisible par model.n_heads")

        # S√©quence maximale ‚â• fen√™tre temporelle demand√©e
        if "max_sequence_length" in m and "sequence" in config:
            if m["max_sequence_length"] < config["sequence"]["months_lookback"]:
                errors.append(
                    "model.max_sequence_length doit √™tre >= sequence.months_lookback"
                )

    # Blocs data : colonnes doivent √™tre pr√©sentes si dataset est fourni
    if "data" in config:
        d = config["data"]
        for col_key in ["client_id_col", "event_time_col", "product_col"]:
            if not d.get(col_key):
                errors.append(f"data.{col_key} est requis (non vide)")

    return errors

# =====================================================================
# Affichage des m√©triques Top-K (pour le notebook)
# =====================================================================

def format_topk_table(metrics_by_k: Dict[int, Dict[str, float]], baseline_metrics: Optional[Dict[int, Dict[str, float]]] = None) -> str:
    """
    Formate joliment les m√©triques Top-K.

    metrics_by_k: {K: {"precision":..., "recall":..., "f1":..., "ndcg":..., "hit_rate":..., "coverage":..., ...}}
    baseline_metrics: m√©triques de r√©f√©rence (optionnel) pour comparaison.
    """
    lines = []
    lines.append("T4REC XLNET - M√âTRIQUES TOP-K")
    lines.append("=" * 80)

    # Ent√™te
    header = "| Metric          |"
    for k in sorted(metrics_by_k.keys()):
        header += f" K={k:<8} |"
    lines.append(header)
    lines.append("|" + "-" * (len(header) - 2) + "|")

    # Liste des cl√©s √† afficher (on mappe depuis nos noms NBO)
    mapping = [
        ("Precision@K", "Precision"),
        ("Recall@K",    "Recall"),
        ("F1@K",        "F1-Score"),
        ("NDCG@K",      "NDCG"),
        ("HitRate@K",   "Hit Rate"),
        ("Coverage@K",  "Coverage"),
    ]

    for key, label in mapping:
        row = f"| {label:<15} |"
        for k in sorted(metrics_by_k.keys()):
            val = metrics_by_k[k].get(key, 0.0) * 100.0
            row += f" {val:>7.2f}% |"
        lines.append(row)

    lines.append("|" + "-" * (len(header) - 2) + "|")
    lines.append("")
    lines.append("üìä INTERPR√âTATION BUSINESS :")

    # Petite synth√®se "meilleur K" par m√©trique
    def _best_k_for(metric_key: str) -> Tuple[int, float]:
        best_k = max(metrics_by_k.keys(), key=lambda kk: metrics_by_k[kk].get(metric_key, 0.0))
        return best_k, metrics_by_k[best_k].get(metric_key, 0.0)

    for key, label in mapping:
        bk, bv = _best_k_for(key)
        lines.append(f"   ‚Üí {label} max √† K={bk}: {bv*100:.2f}%")

    lines.append("")
    lines.append("‚ÑπÔ∏è  D√©finitions :")
    lines.append("   ‚Ä¢ Precision@K : % d‚Äôitems recommand√©s qui sont pertinents")
    lines.append("   ‚Ä¢ Recall@K    : % d‚Äôitems pertinents retrouv√©s dans le Top-K")
    lines.append("   ‚Ä¢ F1@K        : moyenne harmonique pr√©cision / rappel")
    lines.append("   ‚Ä¢ NDCG@K      : qualit√© de l‚Äôordre du ranking")
    lines.append("   ‚Ä¢ Hit Rate@K  : % de clients avec ‚â•1 bon produit dans le Top-K")
    lines.append("   ‚Ä¢ Coverage@K  : % des produits couverts par les reco")
    lines.append("")
    lines.append("   ‚úÖ Powered by T4Rec XLNet")

    return "\n".join(lines)


def print_topk_results(metrics_by_k: Dict[int, Dict[str, float]], baseline_metrics: Optional[Dict[int, Dict[str, float]]] = None) -> None:
    """
    Affiche le tableau Top-K en console (utilis√© par le notebook).
    """
    print(format_topk_table(metrics_by_k, baseline_metrics))

