
# t4rec_toolkit/pipeline_core.py
# -*- coding: utf-8 -*-
"""
PIPELINE HYBRIDE T4Rec + PyTorch avec DIMENSION TEMPORELLE R√âELLE
------------------------------------------------------------------
Ce module :
  1) charge les donn√©es (profil + events) depuis Dataiku,
  2) construit de VRAIES s√©quences temporelles par client (12-24 mois),
  3) pr√©pare les features (cat√©gorielles / s√©quentielles) pour T4Rec,
  4) entra√Æne un Transformer PyTorch sur ces s√©quences,
  5) calcule les m√©triques + sauvegarde datasets (features/pred/metrics/model).

Points cl√©s :
  - Compatible T4Rec 23.04.00 (on n'utilise que les embeddings)
  - Exploite enfin la dimension temporelle (le vrai "+")
  - Regroupement des classes ultra-rares ‚Üí "AUTRES_PRODUITS" (log explicite)
"""

from __future__ import annotations
import logging, time, json, math
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn

# ============== T4Rec (embeddings uniquement) =====================
try:
    import transformers4rec.torch as tr
    from transformers4rec.torch.features.embedding import (
        EmbeddingFeatures,
        FeatureConfig,
        TableConfig,
    )
    from transformers4rec.torch.features.sequence import SequenceEmbeddingFeatures
    _HAS_T4REC = True
    print("T4Rec import OK")
except Exception as e:
    _HAS_T4REC = False
    raise ImportError("T4Rec requis. Installez: pip install transformers4rec[torch]")

# ============== Dataiku (optionnel) ================================
try:
    import dataiku
    _HAS_DATAIKU = True
except Exception:
    _HAS_DATAIKU = False

# ============== TQDM (optionnel) ==================================
try:
    from tqdm.auto import tqdm
    _HAS_TQDM = True
except Exception:
    _HAS_TQDM = False

# ============== Transfos locales ==================================
from .transformers.sequence_transformer import SequenceTransformer  # corrig√©
from .transformers.categorical_transformer import CategoricalTransformer  # corrig√©
from .utils.sequence_builder import SequenceBuilder, SequenceBuilderConfig  # nouveau

# ============== Sklearn (label encoding / metrics) =================
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_recall_fscore_support


# ------------------------------------------------------------------
# 1) CONFIGS & HELPERS
# ------------------------------------------------------------------

@dataclass
class T4RecConfig:
    """Config du mod√®le (dimensions principales)"""
    d_model: int = 256
    n_heads: int = 8
    n_layers: int = 4
    dropout: float = 0.1
    max_sequence_length: int = 24
    vocab_size: int = 2000


def blank_config() -> Dict[str, Any]:
    """
    Gabarit de configuration.
    NOTE: nouvelles cl√©s "events_*" pour activer la dimension temporelle.
    """
    return {
        "data": {
            "dataset_name": "",            # table statique (profil) - optionnelle
            "events_dataset": "",          # table √©v√®nements (obligatoire pour les s√©quences)
            "sample_size": 100_000,        # nb max clients
            "limit": None,
            "chunk_size": 50_000,
            "partitions": None,
            "temporal_split": None,
            # Colonnes (events)
            "client_id_col": "CLIENT_ID",
            "event_time_col": "DATE_EVENT",       # doit √™tre parsable datetime
            "product_col": "PRODUCT_CODE",        # identifiant produit
            # Optionnels : d'autres attributs d'√©v√©nement (canal, montant, etc.)
            "event_extra_cols": [],               # ex: ["CANAL", "MONTANT"]
            # Colonnes profil (si dataset profil est fourni)
            "profile_join_key": "CLIENT_ID",
            "profile_categorical_cols": [],       # ex: ["SEGMENT", "REGION"]
            "profile_sequence_cols": [],          # ex: ["AGE", "REVENU"]
        },
        "sequence": {
            # Fen√™tre temporelle / sampling
            "months_lookback": 24,         # longueur de s√©quence (derniers n mois)
            "min_events_per_client": 1,    # garde les clients avec au moins 1 event
            "time_granularity": "M",       # "M" = mois, "W" = semaine (support de base)
            "target_horizon": 1,           # pr√©dire l'item du mois suivant (horizon=1)
            "pad_value": 0,                # padding index pour les s√©quences
            "build_target_from_events": True,  # True: label=produit du "mois cible"
        },
        "features": {
            "sequence_cols": [],           # (rempli automatiquement pour events)
            "categorical_cols": [],        # (profil ou events extra)
            "target_col": "TARGET_PRODUCT",# label final (si pas build auto)
            "exclude_target_values": ["Aucune_Proposition"],
            "merge_rare_threshold": 200,   # <200 exemples ‚Üí "AUTRES_PRODUITS"
            "other_class_name": "AUTRES_PRODUITS",
        },
        "model": {
            "d_model": 512,
            "n_heads": 8,
            "n_layers": 4,
            "dropout": 0.1,
            "max_sequence_length": 24,
            "vocab_size": 2000,            # base pour embeddings
        },
        "training": {
            "batch_size": 64,
            "num_epochs": 20,
            "learning_rate": 5e-4,
            "weight_decay": 1e-4,
            "val_split": 0.2,
            "class_weighting": True,       # pond√©rer CrossEntropy selon fr√©quences
            "gradient_clip": 1.0,
            "optimizer": "adamw",
        },
        "outputs": {
            "features_dataset": "T4REC_FEATURES",
            "predictions_dataset": "T4REC_PREDICTIONS",
            "metrics_dataset": "T4REC_METRICS",
            "model_artifacts_dataset": "T4REC_MODEL",
            "local_dir": "output",
        },
        "runtime": {
            "verbose": True,
            "progress": True,
            "seed": 42,
        },
    }


def _setup_logging(verbose: bool) -> None:
    """Logging simple."""
    level = logging.INFO if verbose else logging.WARNING
    logging.basicConfig(level=level, format="%(asctime)s - %(levelname)s - %(message)s")


# ------------------------------------------------------------------
# 2) CHARGEMENT & PR√âPARATION DONN√âES
# ------------------------------------------------------------------

def _load_events_df(cfg: Dict[str, Any]) -> pd.DataFrame:
    """
    Charge le dataset d'√©v√©nements depuis :
      - cfg['data']['events_df'] (DataFrame en m√©moire) si fourni
      - sinon un dataset Dataiku (cfg['data']['events_dataset'])
    """
    logger = logging.getLogger(__name__)
    dcfg = cfg["data"]

    # 1) Chemin "no-IO" : DataFrame d√©j√† construit en m√©moire
    if dcfg.get("events_df") is not None:
        df = dcfg["events_df"]
        if not isinstance(df, pd.DataFrame):
            raise ValueError("data.events_df doit √™tre un pandas.DataFrame")
        logger.info(f"Events (in-memory) loaded: shape={df.shape}")
    else:
        # 2) Chemin Dataiku classique
        if not _HAS_DATAIKU:
            raise RuntimeError("Dataiku requis pour charger events_dataset.")
        ds_name = dcfg.get("events_dataset")
        if not ds_name:
            raise ValueError("data.events_dataset est requis (s√©quences temporelles).")
        ds = dataiku.Dataset(ds_name)
        df = ds.get_dataframe(limit=dcfg.get("limit"))
        logger.info(f"Events loaded: {ds_name} ‚Üí {df.shape}")

    # Renommer/valider colonnes
    cid = dcfg["client_id_col"]
    tcol = dcfg["event_time_col"]
    pcol = dcfg["product_col"]
    for c in (cid, tcol, pcol):
        if c not in df.columns:
            raise ValueError(f"Colonne manquante dans events: '{c}'")

    # Date en datetime
    df[tcol] = pd.to_datetime(df[tcol], errors="coerce")
    df = df.dropna(subset=[tcol])
    return df




def _load_profile_df_if_any(cfg: Dict[str, Any]) -> Optional[pd.DataFrame]:
    """
    Charge la table profil :
      1) si cfg["data"]["profile_df"] (DataFrame en m√©moire) est fourni ‚Üí priorit√©
      2) sinon lit le dataset Dataiku cfg["data"]["dataset_name"] si pr√©sent
      3) sinon None
    """
    logger = logging.getLogger(__name__)
    dcfg = cfg["data"]

    # ---- 1) Mode m√©moire ----
    if dcfg.get("profile_df") is not None:
        df = dcfg["profile_df"]
        if not isinstance(df, pd.DataFrame) or df.empty:
            logger.warning("data.profile_df fourni mais vide ou non-DataFrame ‚Üí ignor√©.")
            return None
        logger.info(f"[profile_df] (in-memory) ‚Üí {df.shape}")
        return df

    # ---- 2) Mode Dataiku ----
    if not _HAS_DATAIKU:
        return None

    ds_name = dcfg.get("dataset_name")
    if not ds_name:
        return None

    ds = dataiku.Dataset(ds_name)
    df = ds.get_dataframe(limit=dcfg.get("limit"))
    logger.info(f"Profile loaded: {ds_name} ‚Üí {df.shape}")
    return df



def merge_rare_classes(series: pd.Series, min_count: int, other_name: str) -> Tuple[pd.Series, Dict[str, str]]:
    """
    Regroupe les classes rares (< min_count) en 'other_name'.
    Retourne la s√©rie transform√©e + le mapping classe_originale -> 'other_name'.
    """
    vc = series.value_counts()
    rare = vc[vc < min_count].index
    mapping = {str(x): other_name for x in rare}
    new_series = series.astype(str).where(~series.astype(str).isin(rare), other_name)
    return new_series, mapping


# ------------------------------------------------------------------
# 3) MOD√àLE HYBRIDE
# ------------------------------------------------------------------

class T4RecTemporalModel(nn.Module):
    """
    Mod√®le hybride :
      - Embeddings T4Rec pour les features (s√©quence item_id + extras)
      - TransformerEncoder PyTorch qui lit toute la s√©quence
      - Head de pr√©diction du prochain produit (multi-classes)
    """

    def __init__(
        self,
        embedding_module,           # SequenceEmbeddingFeatures T4Rec
        d_model: int,
        n_heads: int,
        n_layers: int,
        dropout: float,
        n_classes: int,
        max_seq_len: int,
        proj_in_dim: Optional[int] = None,
    ):
        super().__init__()
        self.t4rec_embeddings = embedding_module

        # Si la concat T4Rec ne fait pas "pile" d_model, on projette
        self.proj = None
        if proj_in_dim is not None and proj_in_dim != d_model:
            self.proj = nn.Linear(proj_in_dim, d_model)

        # Positional encoding simple (appris)
        self.positional = nn.Parameter(torch.randn(1, max_seq_len, d_model) * 0.02)

        # Transformer
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            activation="gelu",
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)

        # Head
        self.head = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, n_classes),
        )

        self.d_model = d_model
        self.max_seq_len = max_seq_len

    def forward(self, batch_inputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        batch_inputs = dict de tenseurs Long [B, T] pour toutes les features
        """
        # 1) Embeddings T4Rec (concat par feature ‚Üí [B, T, F_concat])
        x = self.t4rec_embeddings(batch_inputs)  # [B, T, F_concat]

        # 2) Projection √©ventuelle pour matcher d_model
        if self.proj is not None:
            x = self.proj(x)  # [B, T, d_model]

        # 3) + positional
        T = x.shape[1]
        pos = self.positional[:, :T, :]
        x = x + pos

        # 4) Transformer
        x = self.encoder(x)  # [B, T, d_model]

        # 5) On prend le dernier pas de temps comme "r√©sum√©" (next-item)
        out = x[:, -1, :]  # [B, d_model]

        # 6) Logits multi-classes
        logits = self.head(out)  # [B, n_classes]
        return logits


# ------------------------------------------------------------------
# 4) CR√âATION EMBEDDINGS T4REC
# ------------------------------------------------------------------

def _build_t4rec_embedding_module(
    feature_dims: Dict[str, int],
    d_model: int,
    max_seq_len: int,
) -> Tuple[SequenceEmbeddingFeatures, int]:
    """
    Construit un SequenceEmbeddingFeatures o√π chaque feature a sa TableConfig.
    Retourne (module, concat_dim).
    - feature_dims: dict {feature_name: vocab_size}
      Ex: {"item_id": 5000, "canal": 10}
    - Chaque table a une dimension d_model // nb_features (simple & robuste)
    """
    assert _HAS_T4REC
    n_feats = len(feature_dims)
    per_dim = max(8, d_model // max(1, n_feats))  # au moins 8

    feature_cfgs = {}
    for feat, vocab in feature_dims.items():
        tbl = TableConfig(vocabulary_size=vocab, dim=per_dim, name=f"{feat}_table")
        feature_cfgs[feat] = FeatureConfig(
            table=tbl, max_sequence_length=max_seq_len, name=feat
        )

    # item_id = 1√®re feature par convention (obligatoire pour T4Rec)
    item_id = list(feature_dims.keys())[0]
    emb = SequenceEmbeddingFeatures(feature_config=feature_cfgs, item_id=item_id, aggregation="concat")
    concat_dim = per_dim * n_feats
    return emb, concat_dim


# ------------------------------------------------------------------
# 5) ENTRA√éNEMENT
# ------------------------------------------------------------------

def run_training(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Pipeline complet :
      - construction des s√©quences temporelles (SequenceBuilder),
      - encodage des features (cat/seq),
      - embeddings T4Rec + Transformer,
      - training/validation (mini-batches, AMP),
      - m√©triques + sauvegardes.
    """
    t0 = time.time()
    _setup_logging(config["runtime"]["verbose"])
    logger = logging.getLogger(__name__)

    # Seeds
    seed = config["runtime"].get("seed")
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)

    # ---- Device & AMP ----
    use_amp = torch.cuda.is_available()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    scaler = torch.cuda.amp.GradScaler() if use_amp else None

    # 1) Charger donn√©es
    events_df = _load_events_df(config)
    profile_df = _load_profile_df_if_any(config)

    # 2) Construire s√©quences temporelles
    sb_cfg = SequenceBuilderConfig(
        client_id_col=config["data"]["client_id_col"],
        time_col=config["data"]["event_time_col"],
        item_col=config["data"]["product_col"],
        extra_event_cols=config["data"].get("event_extra_cols", []),
        months_lookback=config["sequence"]["months_lookback"],
        time_granularity=config["sequence"]["time_granularity"],
        min_events_per_client=config["sequence"]["min_events_per_client"],
        target_horizon=config["sequence"]["target_horizon"],
        pad_value=config["sequence"]["pad_value"],
        build_target_from_events=config["sequence"]["build_target_from_events"],
    )
    builder = SequenceBuilder(sb_cfg)
    seq_pack = builder.fit_transform(events_df)
    logger.info(
        f"[SequenceBuilder] X_seq keys={list(seq_pack['X_seq'].keys())}, "
        f"shape={next(iter(seq_pack['X_seq'].values())).shape}"
    )

    # 3) Cr√©er/ajouter features profil (facultatif)
    X_cat: Dict[str, np.ndarray] = {}
    X_seq_extra: Dict[str, np.ndarray] = {}
    if profile_df is not None:
        pid = config["data"]["profile_join_key"]
        prof = profile_df.set_index(pid).reindex(seq_pack["client_ids"]).reset_index()
        prof.columns = [pid] + list(prof.columns[1:])

        # Cat√©gorielles
        cat_cols = config["data"].get("profile_categorical_cols", [])
        if len(cat_cols) > 0:
            cat_tr = CategoricalTransformer(handle_unknown="encode", unknown_value=1, name="ProfileCategorical")
            cat_tr.fit(prof, cat_cols)
            cat_res = cat_tr.transform(prof)
            for k, arr in cat_res.data.items():
                X_cat[k.replace("_encoded", "")] = arr.astype(np.int64)

        # "Seq" mono-pas (ex: √¢ge) ‚Üí indices discrets
        seq_cols = config["data"].get("profile_sequence_cols", [])
        if len(seq_cols) > 0:
            seq_tr = SequenceTransformer(name="ProfileSeq")
            seq_tr.fit(prof, seq_cols)
            seq_res = seq_tr.transform(prof)
            for k, arr in seq_res.data.items():
                vocab_p = 200
                arr01 = np.clip(arr, 0.0, 1.0)
                X_seq_extra[k.replace("_seq", "")] = (arr01 * (vocab_p - 1)).astype(np.int64)

    # 4) Cible (y) + exclusion valeurs ind√©sirables
    if config["sequence"]["build_target_from_events"]:
        y_raw = seq_pack["y"]
        y_series = pd.Series(y_raw).astype(str)

        exclude_vals = config["features"].get("exclude_target_values", []) if "features" in config else []
        exclude_vals = [str(x).strip() for x in (exclude_vals or [])]

        y_series_norm = y_series.str.strip()
        mask = ~y_series_norm.isin(exclude_vals)
        n_before = len(y_series_norm)
        n_drop = int((~mask).sum())
        if n_drop > 0:
            logger.info(f"[Target exclusion] {n_drop}/{n_before} lignes exclues (valeurs: {exclude_vals})")
            y_series = y_series[mask]
            for k in list(seq_pack["X_seq"].keys()):
                seq_pack["X_seq"][k] = seq_pack["X_seq"][k][mask.values]
            seq_pack["client_ids"] = np.asarray(seq_pack["client_ids"])[mask.values]
            if X_cat:
                for k in list(X_cat.keys()):
                    X_cat[k] = X_cat[k][mask.values]
            if X_seq_extra:
                for k in list(X_seq_extra.keys()):
                    X_seq_extra[k] = X_seq_extra[k][mask.values]
        else:
            logger.info("[Target exclusion] aucune ligne exclue")
    else:
        raise ValueError("Pour la version temporelle, utilisez build_target_from_events=True.")

    # 5) Regroupement classes rares
    min_count = config["features"]["merge_rare_threshold"]
    other_name = config["features"]["other_class_name"]
    y_merged, rare_map = merge_rare_classes(y_series, min_count=min_count, other_name=other_name)
    if len(rare_map) > 0:
        tops = list(rare_map.keys())[:20]
        logger.info(f"=== Regroupement classes rares ‚Üí '{other_name}' ===")
        logger.info(f"  {len(rare_map)} classes fusionn√©es. Exemples: {tops} ‚Üí {other_name}")
    else:
        logger.info("Aucune classe fusionn√©e (pas de classe < seuil).")

    # 6) Encodage cible
    encoder = LabelEncoder()
    y = encoder.fit_transform(y_merged.values)
    n_classes = len(encoder.classes_)
    logger.info(f"Cible: {n_classes} classes apr√®s fusion √©ventuelle.")

    # 7) Pr√©parer features s√©quentielles ( ‚Üí tenseurs)
    X_seq_dict: Dict[str, torch.Tensor] = {}
    for feat, mat in seq_pack["X_seq"].items():
        X_seq_dict[feat] = torch.tensor(mat, dtype=torch.long)

    # Profils r√©p√©t√©s sur T
    if X_seq_extra:
        T_len = X_seq_dict[list(X_seq_dict.keys())[0]].shape[1]
        for feat, vec in X_seq_extra.items():
            repeated = np.repeat(vec[:, None], repeats=T_len, axis=1)
            X_seq_dict[feat] = torch.tensor(repeated, dtype=torch.long)
    if X_cat:
        T_len = X_seq_dict[list(X_seq_dict.keys())[0]].shape[1]
        for feat, vec in X_cat.items():
            repeated = np.repeat(vec[:, None], repeats=T_len, axis=1)
            X_seq_dict[feat] = torch.tensor(repeated.astype(np.int64), dtype=torch.long)

    # 8) Vocab sizes
    vocab_sizes = dict(seq_pack["vocab_sizes"])
    for feat in X_seq_extra.keys():
        vocab_sizes[feat] = vocab_sizes.get(feat, 200)
    for feat in X_cat.keys():
        vmax = int(np.max(X_cat[feat])) if len(X_cat[feat]) > 0 else 1
        vocab_sizes[feat] = max(vmax + 1, 10)

    # 9) Split train/val (shuffle clients)
    N = len(y)
    idx = np.arange(N); np.random.shuffle(idx)
    split = int(N * (1.0 - config["training"]["val_split"]))
    tr_idx, va_idx = idx[:split], idx[split:]

    def take_idx(d: Dict[str, torch.Tensor], ids: np.ndarray) -> Dict[str, torch.Tensor]:
        return {k: v[ids] for k, v in d.items()}

    Xtr = take_idx(X_seq_dict, tr_idx)
    Xva = take_idx(X_seq_dict, va_idx)
    ytr = torch.tensor(y[tr_idx], dtype=torch.long)
    yva = torch.tensor(y[va_idx], dtype=torch.long)

    # 10) Embeddings T4Rec
    d_model = config["model"]["d_model"]
    max_T = config["model"]["max_sequence_length"]
    emb_mod, concat_dim = _build_t4rec_embedding_module(
        feature_dims=vocab_sizes,
        d_model=d_model,
        max_seq_len=max_T,
    )

    # 11) Mod√®le
    model = T4RecTemporalModel(
        embedding_module=emb_mod,
        d_model=d_model,
        n_heads=config["model"]["n_heads"],
        n_layers=config["model"]["n_layers"],
        dropout=config["model"]["dropout"],
        n_classes=n_classes,
        max_seq_len=max_T,
        proj_in_dim=concat_dim,
    ).to(device)

    # Move datasets to device once
    Xtr = {k: v.to(device) for k, v in Xtr.items()}
    Xva = {k: v.to(device) for k, v in Xva.items()}
    ytr = ytr.to(device)
    yva = yva.to(device)

    logger.info(f"Model params: {sum(p.numel() for p in model.parameters()):,}")

    # 12) Optim / Loss
    if config["training"]["optimizer"].lower() == "adamw":
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config["training"]["learning_rate"],
            weight_decay=config["training"]["weight_decay"],
        )
    else:
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config["training"]["learning_rate"],
            weight_decay=config["training"]["weight_decay"],
        )

    if config["training"]["class_weighting"]:
        counts = np.bincount(ytr.detach().cpu().numpy(), minlength=n_classes).astype(np.float32)
        inv = 1.0 / np.clip(counts, 1.0, None)
        weights = inv / inv.sum() * n_classes
        class_w = torch.tensor(weights, dtype=torch.float32, device=device)
        loss_fn = nn.CrossEntropyLoss(weight=class_w)
    else:
        loss_fn = nn.CrossEntropyLoss()

    # 13) Training loop (mini-batches + AMP)
    num_epochs = config["training"]["num_epochs"]
    bs = config["training"]["batch_size"]
    grad_clip = config["training"].get("gradient_clip", None)

    def iterate_batches(X: Dict[str, torch.Tensor], y: torch.Tensor, batch_size: int):
        Nloc = len(y)
        for s in range(0, Nloc, batch_size):
            e = min(s + batch_size, Nloc)
            yield {k: v[s:e] for k, v in X.items()}, y[s:e]

    pbar = tqdm(range(num_epochs), desc="Training") if (_HAS_TQDM and config["runtime"]["progress"]) else range(num_epochs)
    history = []

    for epoch in pbar:
        model.train()
        tr_loss = 0.0; n_steps = 0
        for xb, yb in iterate_batches(Xtr, ytr, bs):
            optimizer.zero_grad(set_to_none=True)
            if use_amp:
                with torch.cuda.amp.autocast():
                    logits = model(xb)
                    loss = loss_fn(logits, yb)
                scaler.scale(loss).backward()
                if grad_clip:
                    scaler.unscale_(optimizer)
                    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                logits = model(xb)
                loss = loss_fn(logits, yb)
                loss.backward()
                if grad_clip:
                    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
                optimizer.step()
            tr_loss += loss.item(); n_steps += 1
        tr_loss /= max(1, n_steps)

        # Validation (mini-batches)
        model.eval()
        val_losses = []; correct = 0; total = 0
        with torch.no_grad():
            for xb, yb in iterate_batches(Xva, yva, bs):
                if use_amp:
                    with torch.cuda.amp.autocast():
                        logits_b = model(xb)
                        loss_b = loss_fn(logits_b, yb)
                else:
                    logits_b = model(xb)
                    loss_b = loss_fn(logits_b, yb)
                val_losses.append(loss_b.item())
                pred_b = torch.argmax(logits_b, dim=1)
                correct += (pred_b == yb).sum().item()
                total   += len(yb)
        va_loss = float(np.mean(val_losses)) if val_losses else 0.0
        acc     = correct / max(total, 1)

        history.append({"epoch": epoch + 1, "train_loss": tr_loss, "val_loss": va_loss, "val_accuracy": acc})
        if _HAS_TQDM and config["runtime"]["progress"]:
            pbar.set_postfix({"loss": f"{tr_loss:.4f}", "val_acc": f"{acc:.4f}"})
        logger.info(f"Epoch {epoch+1}/{num_epochs} | loss={tr_loss:.4f} | val_loss={va_loss:.4f} | val_acc={acc:.4f}")

    # 14) Final metrics (mini-batches)
    model.eval()
    logits_list = []; yva_list = []; preds_list = []
    with torch.no_grad():
        for xb, yb in iterate_batches(Xva, yva, bs):
            if use_amp:
                with torch.cuda.amp.autocast():
                    logits_b = model(xb)
            else:
                logits_b = model(xb)
            logits_list.append(logits_b)
            yva_list.append(yb)
            preds_list.append(torch.argmax(logits_b, dim=1))

    logits = torch.cat(logits_list, dim=0)
    yva_all = torch.cat(yva_list, dim=0)
    preds = torch.cat(preds_list, dim=0)

    acc = (preds == yva_all).float().mean().item()

    # CPU numpy pour sklearn
    yva_cpu = yva_all.detach().cpu().numpy()
    preds_cpu = preds.detach().cpu().numpy()
    precision, recall, f1, _ = precision_recall_fscore_support(
        yva_cpu, preds_cpu, average="weighted", zero_division=0
    )

    # 15) Top-K (format coll√®gue)
    prediction_scores = logits.detach().cpu().numpy()
    true_labels = yva_all.detach().cpu().numpy()
    inverse_target_mapping = {i: cls for i, cls in enumerate(encoder.classes_)}
    topk_metrics, _ignored = evaluate_topk_metrics_nbo(
        predictions=prediction_scores,
        targets=true_labels,
        inverse_target_mapping=inverse_target_mapping,
        k_values=[1, 3, 5, 10],
    )

    # 16) Sauvegardes Dataiku (best effort)
    saved = {}
    if _HAS_DATAIKU:
        try:
            # PREDICTIONS (client, top1..top5)
            topk = 5
            pred_list = []
            for i, (score, ytrue) in enumerate(zip(prediction_scores, true_labels)):
                top_idx = np.argsort(score)[-topk:][::-1]
                row = {
                    "client_id": int(seq_pack["client_ids"][va_idx[i]]),
                    "true_label": str(inverse_target_mapping.get(int(ytrue), "UNK")),
                    "pred_top1": str(inverse_target_mapping.get(int(top_idx[0]), "UNK")),
                    "pred_top1_score": float(score[top_idx[0]]),
                }
                for k in range(1, topk):
                    ki = top_idx[k]
                    row[f"pred_top{k+1}"] = str(inverse_target_mapping.get(int(ki), "UNK"))
                    row[f"pred_top{k+1}_score"] = float(score[ki])
                pred_list.append(row)

            pred_df = pd.DataFrame(pred_list)
            if config["outputs"].get("predictions_dataset"):
                dataiku.Dataset(config["outputs"]["predictions_dataset"]).write_with_schema(pred_df)
                saved["predictions"] = config["outputs"]["predictions_dataset"]

            # METRICS
            metrics_rows = []
            metrics_rows += [
                {"metric_name": "accuracy", "metric_value": float(acc), "metric_type": "standard", "dataset_split": "validation"},
                {"metric_name": "precision", "metric_value": float(precision), "metric_type": "standard", "dataset_split": "validation"},
                {"metric_name": "recall", "metric_value": float(recall), "metric_type": "standard", "dataset_split": "validation"},
                {"metric_name": "f1", "metric_value": float(f1), "metric_type": "standard", "dataset_split": "validation"},
            ]
            for k, d in topk_metrics.items():
                metrics_rows += [
                    {"metric_name": "Precision@K", "metric_value": float(d.get("Precision@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "Recall@K", "metric_value": float(d.get("Recall@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "F1@K", "metric_value": float(d.get("F1@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "NDCG@K", "metric_value": float(d.get("NDCG@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "MAP", "metric_value": float(d.get("MAP", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "HitRate@K", "metric_value": float(d.get("HitRate@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "Coverage@K", "metric_value": float(d.get("Coverage@K", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                    {"metric_name": "Clients_evaluated", "metric_value": float(d.get("Clients_evaluated", 0.0)), "metric_type": "topk_nbo", "dataset_split": "validation", "k_value": k},
                ]
            met_df = pd.DataFrame(metrics_rows)
            if config["outputs"].get("metrics_dataset"):
                dataiku.Dataset(config["outputs"]["metrics_dataset"]).write_with_schema(met_df)
                saved["metrics"] = config["outputs"]["metrics_dataset"]

            # ARTIFACTS
            artifacts = pd.DataFrame({
                "artifact_name": ["model_config", "sequence_builder_config", "rare_class_mapping_json"],
                "artifact_value": [
                    json.dumps(config["model"]),
                    json.dumps(sb_cfg.__dict__),
                    json.dumps(rare_map, ensure_ascii=False),
                ],
                "timestamp": [pd.Timestamp.now()]*3,
            })
            if config["outputs"].get("model_artifacts_dataset"):
                dataiku.Dataset(config["outputs"]["model_artifacts_dataset"]).write_with_schema(artifacts)
                saved["model_artifacts"] = config["outputs"]["model_artifacts_dataset"]

        except Exception as e:
            logger.warning(f"Sauvegarde Dataiku √©chou√©e (non bloquant): {e}")

    # 17) Retour
    out = {
        "metrics": {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1},
        "predictions": {
            "raw_outputs": prediction_scores,
            "predicted_classes": preds.detach().cpu().numpy(),
            "true_classes": true_labels,
        },
        "model_info": {
            "total_params": int(sum(p.numel() for p in model.parameters())),
            "architecture": f"Hybrid-T4Rec-Transformer {config['model']['n_layers']}L-{config['model']['n_heads']}H-{config['model']['d_model']}D",
        },
        "data_info": {
            "n_clients": int(len(y)),
            "seq_len": int(next(iter(X_seq_dict.values())).shape[1]),
            "features": list(X_seq_dict.keys()),
            "n_classes": int(n_classes),
        },
        "execution_time": float(time.time() - t0),
        "saved_datasets": saved,
    }
    return out



# ------------------------------------------------------------------
# 6) TOP-K (inchang√©, repris de ta version)
# ------------------------------------------------------------------

def compute_ranking_metrics_at_k(client_ids, labels, scores, products, k):
    """
    (copi√© / simplifi√© de ta version ; inchang√© dans l'esprit)
    """
    from sklearn.metrics import ndcg_score, average_precision_score
    from collections import defaultdict
    client_data = defaultdict(list)
    for cid, label, score, prod in zip(client_ids, labels, scores, products):
        client_data[cid].append((label, score, prod))

    ndcgs, aps, recalls, f1s = [], [], [], []
    hit_count = 0
    recommended_products = set()
    precision_topk_total = 0
    topk_count = 0
    valid_clients = 0

    for cid, items in client_data.items():
        y_true = np.array([l for l, _, _ in items], dtype=float)
        y_score = np.array([s for _, s, _ in items], dtype=float)
        y_prods = np.array([p for _, _, p in items])

        if y_true.sum() == 0 or len(y_true) < 2 or np.isnan(y_score).any():
            continue
        valid_clients += 1
        top_k_idx = np.argsort(y_score)[::-1][:k]
        y_topk = y_true[top_k_idx]
        p_topk = y_prods[top_k_idx]
        precision_topk_total += y_topk.sum()
        topk_count += k
        ndcgs.append(ndcg_score([y_true], [y_score], k=k))
        aps.append(average_precision_score(y_true, y_score))
        rec_k = y_topk.sum() / y_true.sum()
        recalls.append(rec_k)
        prec_k = y_topk.sum() / k
        if (prec_k + rec_k) > 0:
            f1s.append(2 * (prec_k * rec_k) / (prec_k + rec_k))
        if y_topk.sum() > 0:
            hit_count += 1
        recommended_products.update(p_topk)

    return {
        "Precision@K": precision_topk_total / topk_count if topk_count > 0 else 0.0,
        "Recall@K": float(np.mean(recalls)) if recalls else 0.0,
        "F1@K": float(np.mean(f1s)) if f1s else 0.0,
        "NDCG@K": float(np.mean(ndcgs)) if ndcgs else 0.0,
        "MAP": float(np.mean(aps)) if aps else 0.0,
        "HitRate@K": hit_count / valid_clients if valid_clients > 0 else 0.0,
        "Coverage@K": len(recommended_products) / len(set(products)) if len(products) > 0 else 0.0,
        "Clients_evaluated": valid_clients,
    }


def convert_predictions_to_nbo_format(predictions, targets, inverse_target_mapping):
    """
    Idem ta version.
    """
    client_ids_list, labels_list, scores_list, products_list = [], [], [], []
    n_samples, n_classes = predictions.shape
    for i in range(n_samples):
        pred = predictions[i]
        true_class = targets[i]
        probs = np.exp(pred) / np.sum(np.exp(pred))
        for j in range(n_classes):
            client_ids_list.append(i + 1)
            labels_list.append(1.0 if j == true_class else 0.0)
            scores_list.append(probs[j])
            products_list.append(inverse_target_mapping.get(j, f"UNK_{j}"))
    return (np.array(client_ids_list), np.array(labels_list), np.array(scores_list), np.array(products_list))


def evaluate_topk_metrics_nbo(predictions, targets, inverse_target_mapping, k_values=[1, 3, 5]):
    """
    Idem ta version : renvoie (metrics_dict, None)
    """
    client_ids, labels, scores, products = convert_predictions_to_nbo_format(
        predictions, targets, inverse_target_mapping
    )
    all_metrics = {}
    for k in k_values:
        all_metrics[k] = compute_ranking_metrics_at_k(client_ids, labels, scores, products, k)
    return all_metrics, None

# =====================================================================
# Validation de la configuration
# =====================================================================

def get_config_schema() -> Dict[str, Any]:
    """
    Sch√©ma minimal pour valider la config.
    NOTE: events_dataset OU events_df doivent √™tre fournis (au moins un).
    """
    return {
        "data": {
            # Au moins l'un des deux :
            "events_dataset": {"type": (str, type(None)), "required": False},
            "events_df": {"type": (pd.DataFrame, type(None)), "required": False},

            # Profil en m√©moire (optionnel)
            "profile_df": {"type": (pd.DataFrame, type(None)), "required": False},

            # Requis pour interpr√©ter les donn√©es
            "client_id_col": {"type": str, "required": True},
            "event_time_col": {"type": str, "required": True},
            "product_col": {"type": str, "required": True},

            # Optionnels
            "dataset_name": {"type": (str, type(None)), "required": False},
            "sample_size": {"type": int, "required": False},
            "limit": {"type": (int, type(None)), "required": False},
            "chunk_size": {"type": int, "required": False},
            "partitions": {"type": (str, list, type(None)), "required": False},
            "temporal_split": {"type": (dict, type(None)), "required": False},
            "event_extra_cols": {"type": list, "required": False},
            "profile_join_key": {"type": (str, type(None)), "required": False},
            "profile_categorical_cols": {"type": list, "required": False},
            "profile_sequence_cols": {"type": list, "required": False},
        },
        "sequence": {
            "months_lookback": {"type": int, "required": True},
            "time_granularity": {"type": str, "required": True},
            "min_events_per_client": {"type": int, "required": True},
            "target_horizon": {"type": int, "required": True},
            "pad_value": {"type": int, "required": True},
            "build_target_from_events": {"type": bool, "required": True},
        },
        "features": {
            "exclude_target_values": {"type": list, "required": False},
            "merge_rare_threshold": {"type": int, "required": True},
            "other_class_name": {"type": str, "required": True},
            "target_col": {"type": (str, type(None)), "required": False},
        },
        "model": {
            "d_model": {"type": int, "required": True},
            "n_heads": {"type": int, "required": True},
            "n_layers": {"type": int, "required": True},
            "dropout": {"type": float, "required": True},
            "max_sequence_length": {"type": int, "required": True},
            "vocab_size": {"type": int, "required": True},
        },
        "training": {
            "batch_size": {"type": int, "required": True},
            "num_epochs": {"type": int, "required": True},
            "learning_rate": {"type": float, "required": True},
            "weight_decay": {"type": float, "required": True},
            "val_split": {"type": float, "required": True},
            "class_weighting": {"type": bool, "required": True},
            "gradient_clip": {"type": (float, type(None)), "required": False},
            "optimizer": {"type": str, "required": True},
        },
        "outputs": {
            "features_dataset": {"type": (str, type(None)), "required": False},
            "predictions_dataset": {"type": (str, type(None)), "required": False},
            "metrics_dataset": {"type": (str, type(None)), "required": False},
            "model_artifacts_dataset": {"type": (str, type(None)), "required": False},
            "local_dir": {"type": str, "required": True},
        },
        "runtime": {
            "verbose": {"type": bool, "required": True},
            "progress": {"type": bool, "required": True},
            "seed": {"type": (int, type(None)), "required": True},
        },
    }



def validate_config(config: Dict[str, Any], strict: bool = False) -> List[str]:
    """
    Valide la config avant entra√Ænement.
    - V√©rifie la pr√©sence et le type de cl√©s essentielles
    - V√©rifie contraintes simples (ex: d_model % n_heads == 0)
    
    Retour:
        Liste d'erreurs (vide si tout est OK)
    """
    errors: List[str] = []
    schema = get_config_schema()

    # Helper pour naviguer dans le dict selon le sch√©ma
    def _check_block(block_name: str, block_schema: Dict[str, Any]):
        if block_name not in config:
            errors.append(f"Bloc '{block_name}' manquant dans la config")
            return
        block = config[block_name]
        for key, spec in block_schema.items():
            required = spec.get("required", False)
            expected_type = spec.get("type", object)
            if required and key not in block:
                errors.append(f"{block_name}.{key} est requis")
                continue
            if key in block and expected_type is not None:
                if not isinstance(block[key], expected_type):
                    errors.append(
                        f"{block_name}.{key} doit √™tre de type {expected_type} (actuel: {type(block[key])})"
                    )

    for section, sect_schema in schema.items():
        _check_block(section, sect_schema)

    # Contraintes sp√©cifiques mod√®le
    if "model" in config:
        m = config["model"]
        if isinstance(m.get("d_model"), int) and isinstance(m.get("n_heads"), int):
            if m["d_model"] % m["n_heads"] != 0:
                errors.append("model.d_model doit √™tre divisible par model.n_heads")

        # S√©quence maximale ‚â• fen√™tre temporelle demand√©e
        if "max_sequence_length" in m and "sequence" in config:
            if m["max_sequence_length"] < config["sequence"]["months_lookback"]:
                errors.append(
                    "model.max_sequence_length doit √™tre >= sequence.months_lookback"
                )


    if "data" in config:
        d = config["data"]

        # Au moins l'un des deux : events_df (DataFrame) OU events_dataset (str non vide)
        has_mem = isinstance(d.get("events_df"), pd.DataFrame) and not d.get("events_df").empty
        has_ds  = isinstance(d.get("events_dataset"), str) and len(d.get("events_dataset")) > 0
        if not (has_mem or has_ds):
            errors.append("data.events_df (DataFrame) ou data.events_dataset (nom de dataset) doit √™tre fourni")

        # Colonnes requises non vides
        for col_key in ["client_id_col", "event_time_col", "product_col"]:
            if not d.get(col_key):
                errors.append(f"data.{col_key} est requis (non vide)")

    return errors

# =====================================================================
# Affichage des m√©triques Top-K (pour le notebook)
# =====================================================================

def format_topk_table(metrics_by_k: Dict[int, Dict[str, float]], baseline_metrics: Optional[Dict[int, Dict[str, float]]] = None) -> str:
    """
    Formate joliment les m√©triques Top-K.

    metrics_by_k: {K: {"precision":..., "recall":..., "f1":..., "ndcg":..., "hit_rate":..., "coverage":..., ...}}
    baseline_metrics: m√©triques de r√©f√©rence (optionnel) pour comparaison.
    """
    lines = []
    lines.append("T4REC XLNET - M√âTRIQUES TOP-K")
    lines.append("=" * 80)

    # Ent√™te
    header = "| Metric          |"
    for k in sorted(metrics_by_k.keys()):
        header += f" K={k:<8} |"
    lines.append(header)
    lines.append("|" + "-" * (len(header) - 2) + "|")

    # Liste des cl√©s √† afficher (on mappe depuis nos noms NBO)
    mapping = [
        ("Precision@K", "Precision"),
        ("Recall@K",    "Recall"),
        ("F1@K",        "F1-Score"),
        ("NDCG@K",      "NDCG"),
        ("HitRate@K",   "Hit Rate"),
        ("Coverage@K",  "Coverage"),
    ]

    for key, label in mapping:
        row = f"| {label:<15} |"
        for k in sorted(metrics_by_k.keys()):
            val = metrics_by_k[k].get(key, 0.0) * 100.0
            row += f" {val:>7.2f}% |"
        lines.append(row)

    lines.append("|" + "-" * (len(header) - 2) + "|")
    lines.append("")
    lines.append("üìä INTERPR√âTATION BUSINESS :")

    # Petite synth√®se "meilleur K" par m√©trique
    def _best_k_for(metric_key: str) -> Tuple[int, float]:
        best_k = max(metrics_by_k.keys(), key=lambda kk: metrics_by_k[kk].get(metric_key, 0.0))
        return best_k, metrics_by_k[best_k].get(metric_key, 0.0)

    for key, label in mapping:
        bk, bv = _best_k_for(key)
        lines.append(f"   ‚Üí {label} max √† K={bk}: {bv*100:.2f}%")

    lines.append("")
    lines.append("‚ÑπÔ∏è  D√©finitions :")
    lines.append("   ‚Ä¢ Precision@K : % d‚Äôitems recommand√©s qui sont pertinents")
    lines.append("   ‚Ä¢ Recall@K    : % d‚Äôitems pertinents retrouv√©s dans le Top-K")
    lines.append("   ‚Ä¢ F1@K        : moyenne harmonique pr√©cision / rappel")
    lines.append("   ‚Ä¢ NDCG@K      : qualit√© de l‚Äôordre du ranking")
    lines.append("   ‚Ä¢ Hit Rate@K  : % de clients avec ‚â•1 bon produit dans le Top-K")
    lines.append("   ‚Ä¢ Coverage@K  : % des produits couverts par les reco")
    lines.append("")
    lines.append("   ‚úÖ Powered by T4Rec XLNet")

    return "\n".join(lines)


def print_topk_results(metrics_by_k: Dict[int, Dict[str, float]], baseline_metrics: Optional[Dict[int, Dict[str, float]]] = None) -> None:
    """
    Affiche le tableau Top-K en console (utilis√© par le notebook).
    """
    print(format_topk_table(metrics_by_k, baseline_metrics))

